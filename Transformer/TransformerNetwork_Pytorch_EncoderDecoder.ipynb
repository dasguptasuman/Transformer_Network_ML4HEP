{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad4fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "import uproot\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn \n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e750104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.18991428 -1.33758012 -0.16379419 ...  0.97133729  0.38098346\n",
      "  -1.97539871]\n",
      " [-0.02620282  0.3494965   0.54694886 ...  0.55314748  0.19741884\n",
      "  -0.15557223]\n",
      " [ 0.70501369  0.95452037  1.         ...  0.82044019  0.71309163\n",
      "   0.61413221]\n",
      " ...\n",
      " [-0.03224206 -0.11911494  0.25985249 ...  0.45905995  0.7742844\n",
      "   1.        ]\n",
      " [ 0.42559843  0.71037992  0.30653368 ...  0.96303223  1.\n",
      "   0.64932407]\n",
      " [-0.13804387  0.03573051 -0.07850811 ...  0.81263918  0.81864257\n",
      "   0.44620173]]\n",
      "Printing y_real\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "### Energy = 0.15 GeV\n",
    "\n",
    "root_file1 = uproot.open(f\"/eos/user/d/dasgupsu/SWAN_projects/ECAL_noise_EM_discrimination/data/outputPSWithPU_withNoise_0.150000_0.15_19Nov_VF1.root\")\n",
    "root_file2 = uproot.open(f\"/eos/user/d/dasgupsu/SWAN_projects/ECAL_noise_EM_discrimination/data/outputPSWithPU_withNoise_0.150000_0.15_19Nov_VF2.root\")\n",
    "tree1 = root_file1[\"Samples\"]\n",
    "tree2 = root_file2[\"Samples\"]\n",
    "arrays1 = tree1.arrays([\"samples\", \"ysamples\",\"samplesNoise\",\"ysamplesNoise\", \"waveform\", \"PUOnlywaveform\"])\n",
    "arrays2 = tree2.arrays([\"samples\", \"ysamples\",\"samplesNoise\",\"ysamplesNoise\", \"PUOnlywaveform\"])\n",
    "X_real = ak.to_numpy(arrays1[\"samples\"])\n",
    "X_real = X_real/np.max(X_real, axis = 1).reshape(200000,1)\n",
    "y_real = ak.to_numpy(arrays1[\"ysamples\"])\n",
    "X_noise = ak.to_numpy(arrays2[\"samplesNoise\"])\n",
    "X_noise = X_noise/np.max(X_noise, axis = 1).reshape(200000,1)\n",
    "y_noise = ak.to_numpy(arrays2[\"ysamplesNoise\"])\n",
    "X_waveform = ak.to_numpy(arrays1[\"waveform\"])\n",
    "X_waveform = X_waveform / np.max(X_waveform, axis = 1).reshape(200000,1)\n",
    "X_Zero = np.zeros((X_waveform.shape[0], X_waveform.shape[1]), dtype=float)\n",
    "\n",
    "print(X_real) \n",
    "print(\"Printing y_real\")\n",
    "print(y_real)\n",
    "\n",
    "data = np.concatenate([X_real, X_noise]) ### makes it [2*num_events,num_samples]\n",
    "#data = X_real\n",
    "labels = np.concatenate([y_real,y_noise])\n",
    "#labels = y_real\n",
    "X_target = np.concatenate([X_waveform, X_Zero])\n",
    "#X_target = X_waveform\n",
    "# Shuffle data and labels together\n",
    "## Important to shuffle since I take some fraction of events so it should not happen that all the real events \n",
    "## are cluttered at the beginning\n",
    "\n",
    "shuffle_indices = np.random.permutation(len(data))\n",
    "data = data[shuffle_indices]\n",
    "labels = labels[shuffle_indices]\n",
    "X_target = X_target[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144f4a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06895913  0.1056133  -0.09143379 ... -0.42161528 -0.29712962\n",
      "  -0.58216445]\n",
      " [ 0.34485138 -0.08731802  0.06661344 ...  0.77302449  0.04550623\n",
      "   0.00307311]\n",
      " [ 0.1237994   0.15758994  0.23702709 ...  0.74946809  0.89637208\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.03092482  0.05597267  0.02970392 ...  0.63602553  0.47977004\n",
      "   0.197776  ]\n",
      " [-0.07213616  0.50158804  0.37323287 ...  0.94731449  1.\n",
      "   0.95559548]\n",
      " [ 0.093267   -0.1169253  -0.21280756 ...  0.9604724   1.\n",
      "   0.49827842]]\n",
      "Printing y_real\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "### Energy = 0.15 GeV\n",
    "root_file = uproot.open(f\"/eos/user/d/dasgupsu/SWAN_projects/ECAL_noise_EM_discrimination/data/outputPSWithPU_withNoise_0.150000_0.15_v11.root\")\n",
    "tree = root_file[\"Samples\"]\n",
    "arrays = tree.arrays([\"samples\", \"ysamples\",\"samplesNoise\",\"ysamplesNoise\", \"waveform\"])\n",
    "X_real = ak.to_numpy(arrays[\"samples\"])\n",
    "y_real = ak.to_numpy(arrays[\"ysamples\"])\n",
    "X_noise = ak.to_numpy(arrays[\"samplesNoise\"])\n",
    "y_noise = ak.to_numpy(arrays[\"ysamplesNoise\"])\n",
    "X_waveform = ak.to_numpy(arrays[\"waveform\"])\n",
    "X_Zero = np.zeros((X_waveform.shape[0], X_waveform.shape[1]), dtype=float)\n",
    "\n",
    "print(X_real) \n",
    "print(\"Printing y_real\")\n",
    "print(y_real)\n",
    "\n",
    "data = np.concatenate([X_real, X_noise]) ### makes it [2*num_events,num_samples]\n",
    "#data = X_real\n",
    "labels = np.concatenate([y_real,y_noise])\n",
    "#labels = y_real\n",
    "X_target = np.concatenate([X_waveform, X_Zero])\n",
    "#X_target = X_waveform\n",
    "# Shuffle data and labels together\n",
    "## Important to shuffle since I take some fraction of events so it should not happen that all the real events \n",
    "## are cluttered at the beginning\n",
    "\n",
    "shuffle_indices = np.random.permutation(len(data))\n",
    "data = data[shuffle_indices]\n",
    "labels = labels[shuffle_indices]\n",
    "X_target = X_target[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decee919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in data is 400000\n",
      "number of rows in data is 10\n",
      "Size of training data is 200000\n",
      "number of elements in data : training data : test data : test target : train target: 400000 : 200000 : 200000: 200000: 200000\n"
     ]
    }
   ],
   "source": [
    "import awkward as ak\n",
    "\n",
    "num_events_data = ak.num(data, axis=0)\n",
    "print(f'number of rows in data is {num_events_data}')\n",
    "\n",
    "\n",
    "ntimeSamples_data = ak.num(data, axis=1)\n",
    "print(f'number of rows in data is {ntimeSamples_data[0]}') ## just take the 0th event\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(0.5 * num_events_data) ###times 2 because the noise is also in the same dataset, so it is 2*num_events\n",
    "train_data = data[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "train_target = X_target[:train_size]\n",
    "test_data = data[train_size:]\n",
    "test_labels = labels[train_size:]\n",
    "test_target = X_target[train_size:]\n",
    "print(f'Size of training data is {train_size}')\n",
    "\n",
    "'''\n",
    "print(train_size)\n",
    "print(train_data)\n",
    "print(test_data)\n",
    "'''\n",
    "\n",
    "print(f'number of elements in data : training data : test data : test target : train target: {len(data)} : {len(train_data)} : {len(test_data)}: {len(test_target)}: {len(train_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c747d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reshape((train_data.shape[0], train_data.shape[1], 1))\n",
    "train_target = train_target.reshape((train_target.shape[0], train_target.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbba1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.reshape((test_data.shape[0], test_data.shape[1], 1))\n",
    "test_target = test_target.reshape((test_target.shape[0], test_target.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0c4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.reshape((train_labels.shape[0], 1))\n",
    "test_labels = test_labels.reshape((test_labels.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f6b2296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be65f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_labels, dtype =torch.float).to(device)\n",
    "test_labels = torch.tensor(test_labels, dtype =torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4956d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_target[59904]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f21ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data, dtype =torch.float).to(device)\n",
    "train_target = torch.tensor(train_target, dtype =torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c97d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.tensor(test_data, dtype =torch.float).to(device)\n",
    "test_target = torch.tensor(test_target, dtype =torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc00706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200000, 10, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be49af7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200000, 10, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71531c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for transformer models.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        data: torch.tensor,\n",
    "        target: torch.tensor,\n",
    "        device\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.device = device\n",
    "\n",
    "        print(\"data size = {}\".format(data.size()))\n",
    "        print(\"target size = {}\".format(target.size()))\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data.size()[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple with 3 elements:\n",
    "        1) src (the encoder input)\n",
    "        2) trg (the decoder input)\n",
    "        3) trg_y (the target)\n",
    "        \"\"\"\n",
    "        #print(self.data.size())\n",
    "        src = self.data[index]\n",
    "        \n",
    "        start_token = 99.*torch.ones((self.target.size()[0],1,1)).to(self.device)\n",
    "        target = torch.cat((start_token, self.target),1)[index]\n",
    "        \n",
    "        trg = target[:-1,:]\n",
    "        trg_y = target[1:,:]\n",
    "        \n",
    "\n",
    "        return src, trg, trg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cce949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size = torch.Size([200000, 10, 1])\n",
      "target size = torch.Size([200000, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "training_data = TransformerDataset(\n",
    "    data = train_data,\n",
    "    target = train_target,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "802684e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size = torch.Size([200000, 10, 1])\n",
      "target size = torch.Size([200000, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "testing_data = TransformerDataset(\n",
    "    data = test_data,\n",
    "    target = test_target,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ced4fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dropout: float=0.1, \n",
    "        max_seq_len: int=10, \n",
    "        d_model: int=32,\n",
    "        batch_first: bool=True\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # adapted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        if self.batch_first:\n",
    "            pe = torch.zeros(1, max_seq_len, d_model)\n",
    "            \n",
    "            pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "            \n",
    "            pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        \n",
    "            pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "            pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or \n",
    "               [enc_seq_len, batch_size, dim_val]\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            x = x + self.pe[:,:x.size(1)]\n",
    "        else:\n",
    "            x = x + self.pe[:x.size(0)]\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1db402a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(dim1: int, dim2: int, device) -> Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "        dim1: int,  target sequence length\n",
    "\n",
    "        dim2: int, for src masking this must be encoder sequence length (i.e. \n",
    "              the length of the input sequence to the model), \n",
    "              and for tgt masking, this must be target sequence length \n",
    "\n",
    "\n",
    "    Return:\n",
    "\n",
    "        A Tensor of shape [dim1, dim2]\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "430600dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        input_size: int,\n",
    "        dec_seq_len: int,\n",
    "        batch_first: bool,\n",
    "        out_seq_len: int=10,\n",
    "        dim_val: int=32,  \n",
    "        n_encoder_layers: int=2,\n",
    "        n_decoder_layers: int=2,\n",
    "        n_heads: int=8,\n",
    "        dropout_encoder: float=0.1, \n",
    "        dropout_decoder: float=0.1,\n",
    "        dropout_pos_enc: float=0.1,\n",
    "        dim_feedforward_encoder: int=128,\n",
    "        dim_feedforward_decoder: int=128\n",
    "        ): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce \n",
    "                     outputs of dimension dim_val\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "            dropout_decoder: float, the dropout rate of the decoder\n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer \n",
    "                                     of the encoder\n",
    "            dim_feedforward_decoder: int, number of neurons in the linear layer \n",
    "                                     of the decoder\n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    Most of the time, this will be 1 .\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=dim_val \n",
    "            )\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=input_size,\n",
    "            out_features=dim_val\n",
    "            )  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=dim_val, \n",
    "            out_features=input_size\n",
    "            )\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoder(\n",
    "            d_model=dim_val,\n",
    "            dropout=dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        # Creating the encoder layer \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerEncoder\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the decoder layers in nn.TransformerDecoder\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, \n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \n",
    "        Args:\n",
    "            src: the encoder's output sequence. Shape: \n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if \n",
    "                 batch_first=True, where S is the source sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            tgt: the sequence to the decoder. Shape: \n",
    "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if \n",
    "                 batch_first=True, where T is the target sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            src_mask: the mask for the src sequence to prevent the model from \n",
    "                      using data points from the target sequence\n",
    "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
    "                      using data points from the target sequence\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
    "        #print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "\n",
    "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "            )\n",
    "        #print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "\n",
    "        # Pass decoder input through decoder input layer\n",
    "        decoder_output = self.decoder_input_layer(tgt) # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\n",
    "\n",
    "        #if src_mask is not None:\n",
    "            #print(\"From model.forward(): Size of src_mask: {}\".format(src_mask.size()))\n",
    "        #if tgt_mask is not None:\n",
    "            #print(\"From model.forward(): Size of tgt_mask: {}\".format(tgt_mask.size()))\n",
    "\n",
    "        # Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "\n",
    "        #print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\n",
    "\n",
    "        # Pass through linear mapping\n",
    "        decoder_output = self.linear_mapping(decoder_output) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): decoder_output size after linear_mapping = {}\".format(decoder_output.size()))\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd189d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_encoder_decoder_inference(\n",
    "    model: nn.Module, \n",
    "    src: torch.Tensor, \n",
    "    batch_size: int,\n",
    "    device,\n",
    "    output_sequence_length: int,\n",
    "    batch_first: bool=True\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        model: An encoder-decoder type model where the decoder requires\n",
    "               target values as input. Should be set to evaluation mode before \n",
    "               passed to this function.\n",
    "               \n",
    "        src: The input to the model\n",
    "        \n",
    "        output_sequence_length: The desired length of the model's output\n",
    "        \n",
    "        batch_size: batch size\n",
    "        \n",
    "        batch_first: If true, the shape of the model input should be \n",
    "                     [batch size, input sequence length, number of features].\n",
    "                     If false, [input sequence length, batch size, number of features]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Dimension of a batched model input that contains the target sequence values\n",
    "    target_seq_dim = 0 if batch_first == False else 1\n",
    "\n",
    "    # Take the last value of thetarget variable in all batches in src and make it tgt\n",
    "    # as per the Influenza paper\n",
    "    tgt = 99.0*torch.ones(1, batch_size, 1).to(device) if batch_first == False else 99.0*torch.ones(batch_size, 1, 1).to(device) # shape [1, batch_size, 1]\n",
    "\n",
    "    # Iteratively concatenate tgt with the first element in the prediction\n",
    "    for _ in range(output_sequence_length-1):\n",
    "\n",
    "        # Create masks\n",
    "        dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "        dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_a,\n",
    "            device = device\n",
    "            )\n",
    "\n",
    "        src_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_b,\n",
    "            device = device\n",
    "            )\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model(src, tgt, src_mask, tgt_mask) \n",
    "\n",
    "        # If statement simply makes sure that the predicted value is \n",
    "        # extracted and reshaped correctly\n",
    "        if batch_first == False:\n",
    "\n",
    "            # Obtain the predicted value at t+1 where t is the last time step \n",
    "            # represented in tgt\n",
    "            last_predicted_value = prediction[-1, :, :] \n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [1, batch_size, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Obtain predicted value\n",
    "            last_predicted_value = prediction[:, -1, :]\n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [batch_size, 1, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(-1)\n",
    "\n",
    "        # Detach the predicted element from the graph and concatenate with \n",
    "        # tgt in dimension 1 or 0\n",
    "        tgt = torch.cat((tgt, last_predicted_value.detach()), target_seq_dim)\n",
    "    \n",
    "    # Create masks\n",
    "    dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "    dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_a,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_b,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    # Make final prediction\n",
    "    final_prediction = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f71d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.20508991181850433\n",
      "training_loss: 0.026662617921829224\n",
      "training_loss: 0.017307737842202187\n",
      "training_loss: 0.018968436866998672\n",
      "training_loss: 0.014871898107230663\n",
      "training_loss: 0.011098555289208889\n",
      "training_loss: 0.01351023931056261\n",
      "training_loss: 0.013439861126244068\n",
      "training_loss: 0.008108141832053661\n",
      "training_loss: 0.01564137078821659\n",
      "training_loss: 0.011046512983739376\n",
      "training_loss: 0.012494734488427639\n",
      "training_loss: 0.009348331019282341\n",
      "training_loss: 0.007781487889587879\n",
      "training_loss: 0.00853805709630251\n",
      "training_loss: 0.012718334794044495\n",
      "training_loss: 0.00968895386904478\n",
      "training_loss: 0.00941701140254736\n",
      "training_loss: 0.008288857527077198\n",
      "training_loss: 0.01326378621160984\n",
      "training_loss: 0.010231973603367805\n",
      "training_loss: 0.012056143954396248\n",
      "training_loss: 0.011955492198467255\n",
      "training_loss: 0.008097621612250805\n",
      "training_loss: 0.016117991879582405\n",
      "training_loss: 0.011407926678657532\n",
      "training_loss: 0.009318269789218903\n",
      "training_loss: 0.00986369326710701\n",
      "training_loss: 0.010316113941371441\n",
      "training_loss: 0.010226277634501457\n",
      "training_loss: 0.010771817527711391\n",
      "training_loss: 0.00976010411977768\n",
      " epoch:0    training_loss: 0.011440718546509743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_104a_cuda/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /build/jenkins/workspace/lcg_release_pipeline/build/pyexternals/torch-2.0.1p1/src/torch/2.0.1p1/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_loss: 0.25431519746780396\n",
      "testing_loss: 0.2264908403158188\n",
      "testing_loss: 0.1808394491672516\n",
      "testing_loss: 0.22060167789459229\n",
      "testing_loss: 0.20873935520648956\n",
      "testing_loss: 0.22994089126586914\n",
      "testing_loss: 0.22018848359584808\n",
      "testing_loss: 0.22699718177318573\n",
      "testing_loss: 0.22459565103054047\n",
      "testing_loss: 0.2586626708507538\n",
      "testing_loss: 0.20647208392620087\n",
      "testing_loss: 0.25034117698669434\n",
      "testing_loss: 0.23026371002197266\n",
      "testing_loss: 0.22844906151294708\n",
      "testing_loss: 0.2024151086807251\n",
      "testing_loss: 0.18600164353847504\n",
      "testing_loss: 0.1828191578388214\n",
      "testing_loss: 0.24065151810646057\n",
      "testing_loss: 0.24463175237178802\n",
      "testing_loss: 0.18505923449993134\n",
      "testing_loss: 0.23365965485572815\n",
      "testing_loss: 0.24527552723884583\n",
      "testing_loss: 0.18920817971229553\n",
      "testing_loss: 0.21717654168605804\n",
      "testing_loss: 0.22192688286304474\n",
      "testing_loss: 0.2254897803068161\n",
      "testing_loss: 0.18210381269454956\n",
      "testing_loss: 0.16268210113048553\n",
      "testing_loss: 0.2062913477420807\n",
      "testing_loss: 0.1963212937116623\n",
      "testing_loss: 0.2369242012500763\n",
      "testing_loss: 0.24995605647563934\n",
      " epoch:0    testing_loss: 0.243003711104393\n",
      "tensor([[0.2132],\n",
      "        [0.1706],\n",
      "        [0.1315],\n",
      "        [0.1140],\n",
      "        [0.6475],\n",
      "        [1.0000],\n",
      "        [0.9900],\n",
      "        [0.8118],\n",
      "        [0.6177],\n",
      "        [0.4758]], device='cuda:0') :::: tensor([[0.4421],\n",
      "        [0.4711],\n",
      "        [0.4493],\n",
      "        [0.4087],\n",
      "        [0.8523],\n",
      "        [1.0432],\n",
      "        [0.9523],\n",
      "        [0.7964],\n",
      "        [0.6212],\n",
      "        [0.5099]], device='cuda:0')\n",
      "tensor([[0.2863],\n",
      "        [0.2411],\n",
      "        [0.1896],\n",
      "        [0.1698],\n",
      "        [0.7299],\n",
      "        [1.0000],\n",
      "        [0.9310],\n",
      "        [0.7430],\n",
      "        [0.6203],\n",
      "        [0.8853]], device='cuda:0') :::: tensor([[0.1169],\n",
      "        [0.1348],\n",
      "        [0.1474],\n",
      "        [0.2214],\n",
      "        [0.7810],\n",
      "        [1.0433],\n",
      "        [0.9700],\n",
      "        [0.8105],\n",
      "        [0.6463],\n",
      "        [0.5400]], device='cuda:0')\n",
      "training_loss: 0.007054378278553486\n",
      "training_loss: 0.007124531548470259\n",
      "training_loss: 0.006088090594857931\n",
      "training_loss: 0.00905065517872572\n",
      "training_loss: 0.007241022773087025\n",
      "training_loss: 0.007117220666259527\n",
      "training_loss: 0.006811122875660658\n",
      "training_loss: 0.00830357987433672\n",
      "training_loss: 0.005286833271384239\n",
      "training_loss: 0.008842965587973595\n",
      "training_loss: 0.007398134563118219\n",
      "training_loss: 0.00835675559937954\n",
      "training_loss: 0.00679116090759635\n",
      "training_loss: 0.005108134355396032\n",
      "training_loss: 0.0055780415423214436\n",
      "training_loss: 0.009332926943898201\n",
      "training_loss: 0.007363435812294483\n",
      "training_loss: 0.007945224642753601\n",
      "training_loss: 0.006706468760967255\n",
      "training_loss: 0.01079472154378891\n",
      "training_loss: 0.00861610472202301\n",
      "training_loss: 0.009556815959513187\n",
      "training_loss: 0.008729859255254269\n",
      "training_loss: 0.00621067825704813\n",
      "training_loss: 0.00860779732465744\n",
      "training_loss: 0.01050516963005066\n",
      "training_loss: 0.008786341175436974\n",
      "training_loss: 0.014100858941674232\n",
      "training_loss: 0.007990715093910694\n",
      "training_loss: 0.00851075816899538\n",
      "training_loss: 0.007550274487584829\n",
      "training_loss: 0.007725962903350592\n",
      " epoch:1    training_loss: 0.010229635052382946\n",
      "testing_loss: 0.23649023473262787\n",
      "testing_loss: 0.2009415179491043\n",
      "testing_loss: 0.1647198647260666\n",
      "testing_loss: 0.16259220242500305\n",
      "testing_loss: 0.17702704668045044\n",
      "testing_loss: 0.20636987686157227\n",
      "testing_loss: 0.19076478481292725\n",
      "testing_loss: 0.18854254484176636\n",
      "testing_loss: 0.2118539661169052\n",
      "testing_loss: 0.22511625289916992\n",
      "testing_loss: 0.16890199482440948\n",
      "testing_loss: 0.2246645987033844\n",
      "testing_loss: 0.19457954168319702\n",
      "testing_loss: 0.17940066754817963\n",
      "testing_loss: 0.16780272126197815\n",
      "testing_loss: 0.16644351184368134\n",
      "testing_loss: 0.1553836613893509\n",
      "testing_loss: 0.19047394394874573\n",
      "testing_loss: 0.225579172372818\n",
      "testing_loss: 0.1675233244895935\n",
      "testing_loss: 0.19860801100730896\n",
      "testing_loss: 0.2151607722043991\n",
      "testing_loss: 0.1617097109556198\n",
      "testing_loss: 0.20262007415294647\n",
      "testing_loss: 0.18960607051849365\n",
      "testing_loss: 0.18580861389636993\n",
      "testing_loss: 0.15455929934978485\n",
      "testing_loss: 0.1297164410352707\n",
      "testing_loss: 0.14671030640602112\n",
      "testing_loss: 0.16694405674934387\n",
      "testing_loss: 0.2039799690246582\n",
      "testing_loss: 0.20524962246418\n",
      " epoch:1    testing_loss: 0.20106148719787598\n",
      "tensor([[0.2132],\n",
      "        [0.1706],\n",
      "        [0.1315],\n",
      "        [0.1140],\n",
      "        [0.6475],\n",
      "        [1.0000],\n",
      "        [0.9900],\n",
      "        [0.8118],\n",
      "        [0.6177],\n",
      "        [0.4758]], device='cuda:0') :::: tensor([[0.4005],\n",
      "        [0.4055],\n",
      "        [0.3737],\n",
      "        [0.3491],\n",
      "        [0.8102],\n",
      "        [1.0190],\n",
      "        [0.9274],\n",
      "        [0.7713],\n",
      "        [0.6074],\n",
      "        [0.4591]], device='cuda:0')\n",
      "tensor([[0.2863],\n",
      "        [0.2411],\n",
      "        [0.1896],\n",
      "        [0.1698],\n",
      "        [0.7299],\n",
      "        [1.0000],\n",
      "        [0.9310],\n",
      "        [0.7430],\n",
      "        [0.6203],\n",
      "        [0.8853]], device='cuda:0') :::: tensor([[0.1404],\n",
      "        [0.1737],\n",
      "        [0.2181],\n",
      "        [0.3207],\n",
      "        [0.7877],\n",
      "        [1.0159],\n",
      "        [0.9221],\n",
      "        [0.7586],\n",
      "        [0.6295],\n",
      "        [0.5175]], device='cuda:0')\n",
      "training_loss: 0.006718622054904699\n",
      "training_loss: 0.006620438303798437\n",
      "training_loss: 0.005430757533758879\n",
      "training_loss: 0.008529619313776493\n",
      "training_loss: 0.0066458615474402905\n",
      "training_loss: 0.007092697080224752\n",
      "training_loss: 0.0080699622631073\n",
      "training_loss: 0.009316365234553814\n",
      "training_loss: 0.004971276968717575\n",
      "training_loss: 0.008097795769572258\n",
      "training_loss: 0.007269668858498335\n",
      "training_loss: 0.007855929434299469\n",
      "training_loss: 0.0063656228594481945\n",
      "training_loss: 0.0050253006629645824\n",
      "training_loss: 0.005485003348439932\n",
      "training_loss: 0.008965357206761837\n",
      "training_loss: 0.008081737905740738\n",
      "training_loss: 0.007806444074958563\n",
      "training_loss: 0.0069242240861058235\n",
      "training_loss: 0.008336166851222515\n",
      "training_loss: 0.008070039562880993\n",
      "training_loss: 0.009339761920273304\n",
      "training_loss: 0.014474880881607533\n",
      "training_loss: 0.008615571074187756\n",
      "training_loss: 0.008596043102443218\n",
      "training_loss: 0.011427587829530239\n",
      "training_loss: 0.008956650272011757\n",
      "training_loss: 0.006983211729675531\n",
      "training_loss: 0.00787592027336359\n",
      "training_loss: 0.008568111807107925\n",
      "training_loss: 0.007228975184261799\n",
      "training_loss: 0.007207942195236683\n",
      " epoch:2    training_loss: 0.010262242518365383\n",
      "testing_loss: 0.21642270684242249\n",
      "testing_loss: 0.19412709772586823\n",
      "testing_loss: 0.15689264237880707\n",
      "testing_loss: 0.14956407248973846\n",
      "testing_loss: 0.17350195348262787\n",
      "testing_loss: 0.19341973960399628\n",
      "testing_loss: 0.17648552358150482\n",
      "testing_loss: 0.17989763617515564\n",
      "testing_loss: 0.19041766226291656\n",
      "testing_loss: 0.198228120803833\n",
      "testing_loss: 0.16094768047332764\n",
      "testing_loss: 0.20740841329097748\n",
      "testing_loss: 0.1985984891653061\n",
      "testing_loss: 0.17780272662639618\n",
      "testing_loss: 0.1575339436531067\n",
      "testing_loss: 0.1585511416196823\n",
      "testing_loss: 0.15257559716701508\n",
      "testing_loss: 0.17589189112186432\n",
      "testing_loss: 0.2073853462934494\n",
      "testing_loss: 0.1571989357471466\n",
      "testing_loss: 0.18523824214935303\n",
      "testing_loss: 0.1994709074497223\n",
      "testing_loss: 0.17110304534435272\n",
      "testing_loss: 0.19051557779312134\n",
      "testing_loss: 0.19444863498210907\n",
      "testing_loss: 0.1663382351398468\n",
      "testing_loss: 0.15199685096740723\n",
      "testing_loss: 0.11767204105854034\n",
      "testing_loss: 0.13018406927585602\n",
      "testing_loss: 0.15008866786956787\n",
      "testing_loss: 0.19357983767986298\n",
      "testing_loss: 0.1827990859746933\n",
      " epoch:2    testing_loss: 0.1942346692085266\n",
      "tensor([[0.2132],\n",
      "        [0.1706],\n",
      "        [0.1315],\n",
      "        [0.1140],\n",
      "        [0.6475],\n",
      "        [1.0000],\n",
      "        [0.9900],\n",
      "        [0.8118],\n",
      "        [0.6177],\n",
      "        [0.4758]], device='cuda:0') :::: tensor([[0.3818],\n",
      "        [0.3526],\n",
      "        [0.2933],\n",
      "        [0.2658],\n",
      "        [0.7679],\n",
      "        [0.9941],\n",
      "        [0.9204],\n",
      "        [0.7635],\n",
      "        [0.6197],\n",
      "        [0.5120]], device='cuda:0')\n",
      "tensor([[0.2863],\n",
      "        [0.2411],\n",
      "        [0.1896],\n",
      "        [0.1698],\n",
      "        [0.7299],\n",
      "        [1.0000],\n",
      "        [0.9310],\n",
      "        [0.7430],\n",
      "        [0.6203],\n",
      "        [0.8853]], device='cuda:0') :::: tensor([[0.1589],\n",
      "        [0.1659],\n",
      "        [0.1743],\n",
      "        [0.2376],\n",
      "        [0.7592],\n",
      "        [0.9894],\n",
      "        [0.9283],\n",
      "        [0.7791],\n",
      "        [0.6604],\n",
      "        [0.5436]], device='cuda:0')\n",
      "training_loss: 0.006382996682077646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.005724689457565546\n",
      "training_loss: 0.005108800251036882\n",
      "training_loss: 0.00863172672688961\n",
      "training_loss: 0.006396034266799688\n",
      "training_loss: 0.007158649154007435\n",
      "training_loss: 0.006907624192535877\n",
      "training_loss: 0.00787464901804924\n",
      "training_loss: 0.004744329955428839\n",
      "training_loss: 0.008053027093410492\n",
      "training_loss: 0.00682167848572135\n",
      "training_loss: 0.007851406931877136\n",
      "training_loss: 0.006235026754438877\n",
      "training_loss: 0.004858965519815683\n",
      "training_loss: 0.00946882925927639\n",
      "training_loss: 0.00907520204782486\n",
      "training_loss: 0.008493897505104542\n",
      "training_loss: 0.0077934712171554565\n",
      "training_loss: 0.005628831684589386\n",
      "training_loss: 0.007998392917215824\n",
      "training_loss: 0.008192215114831924\n",
      "training_loss: 0.009230010211467743\n",
      "training_loss: 0.008383330889046192\n",
      "training_loss: 0.006020635366439819\n",
      "training_loss: 0.00823049433529377\n",
      "training_loss: 0.010592779144644737\n",
      "training_loss: 0.008702739141881466\n",
      "training_loss: 0.00665506673976779\n",
      "training_loss: 0.007311907596886158\n",
      "training_loss: 0.007462158799171448\n",
      "training_loss: 0.01116595882922411\n",
      "training_loss: 0.007318580057471991\n",
      " epoch:3    training_loss: 0.010580591857433319\n",
      "testing_loss: 0.19416578114032745\n",
      "testing_loss: 0.17381440103054047\n",
      "testing_loss: 0.14601096510887146\n",
      "testing_loss: 0.1297583281993866\n",
      "testing_loss: 0.14018641412258148\n",
      "testing_loss: 0.19193445146083832\n",
      "testing_loss: 0.15467718243598938\n",
      "testing_loss: 0.14876054227352142\n",
      "testing_loss: 0.17142823338508606\n",
      "testing_loss: 0.17887990176677704\n",
      "testing_loss: 0.14009615778923035\n",
      "testing_loss: 0.18071173131465912\n",
      "testing_loss: 0.1506914347410202\n",
      "testing_loss: 0.14480780065059662\n",
      "testing_loss: 0.14185547828674316\n",
      "testing_loss: 0.12160523980855942\n",
      "testing_loss: 0.1464117467403412\n",
      "testing_loss: 0.14205846190452576\n",
      "testing_loss: 0.17013394832611084\n",
      "testing_loss: 0.16328482329845428\n",
      "testing_loss: 0.16431310772895813\n",
      "testing_loss: 0.1852586567401886\n",
      "testing_loss: 0.16362212598323822\n",
      "testing_loss: 0.17006276547908783\n",
      "testing_loss: 0.1653803288936615\n",
      "testing_loss: 0.14119358360767365\n",
      "testing_loss: 0.1534494310617447\n",
      "testing_loss: 0.10499724000692368\n",
      "testing_loss: 0.11141295731067657\n",
      "testing_loss: 0.15616893768310547\n",
      "testing_loss: 0.17629078030586243\n",
      "testing_loss: 0.16664712131023407\n",
      " epoch:3    testing_loss: 0.15929977595806122\n",
      "tensor([[0.2132],\n",
      "        [0.1706],\n",
      "        [0.1315],\n",
      "        [0.1140],\n",
      "        [0.6475],\n",
      "        [1.0000],\n",
      "        [0.9900],\n",
      "        [0.8118],\n",
      "        [0.6177],\n",
      "        [0.4758]], device='cuda:0') :::: tensor([[0.4254],\n",
      "        [0.3853],\n",
      "        [0.3265],\n",
      "        [0.3248],\n",
      "        [0.8142],\n",
      "        [1.0142],\n",
      "        [0.9159],\n",
      "        [0.7442],\n",
      "        [0.5894],\n",
      "        [0.4698]], device='cuda:0')\n",
      "tensor([[0.2863],\n",
      "        [0.2411],\n",
      "        [0.1896],\n",
      "        [0.1698],\n",
      "        [0.7299],\n",
      "        [1.0000],\n",
      "        [0.9310],\n",
      "        [0.7430],\n",
      "        [0.6203],\n",
      "        [0.8853]], device='cuda:0') :::: tensor([[0.1577],\n",
      "        [0.1523],\n",
      "        [0.1638],\n",
      "        [0.2351],\n",
      "        [0.7485],\n",
      "        [1.0090],\n",
      "        [0.9231],\n",
      "        [0.7577],\n",
      "        [0.6218],\n",
      "        [0.4848]], device='cuda:0')\n",
      "training_loss: 0.0064184824004769325\n",
      "training_loss: 0.005852534901350737\n",
      "training_loss: 0.004839369561523199\n",
      "training_loss: 0.008435911498963833\n",
      "training_loss: 0.006348912604153156\n",
      "training_loss: 0.008268706500530243\n",
      "training_loss: 0.0074606179259717464\n",
      "training_loss: 0.007777127902954817\n",
      "training_loss: 0.005976485554128885\n",
      "training_loss: 0.008241754956543446\n",
      "training_loss: 0.0067261080257594585\n",
      "training_loss: 0.007719373796135187\n",
      "training_loss: 0.0063608004711568356\n",
      "training_loss: 0.005205805413424969\n",
      "training_loss: 0.005435877945274115\n",
      "training_loss: 0.00880134291946888\n",
      "training_loss: 0.007365207653492689\n",
      "training_loss: 0.007789796683937311\n",
      "training_loss: 0.005669063422828913\n",
      "training_loss: 0.007947216741740704\n",
      "training_loss: 0.008393614552915096\n",
      "training_loss: 0.009153999388217926\n",
      "training_loss: 0.008186453953385353\n",
      "training_loss: 0.0060719954781234264\n",
      "training_loss: 0.008395778015255928\n",
      "training_loss: 0.010683196596801281\n",
      "training_loss: 0.007879025302827358\n",
      "training_loss: 0.007012067828327417\n",
      "training_loss: 0.007226719055324793\n",
      "training_loss: 0.007594726514071226\n",
      "training_loss: 0.006933204364031553\n",
      "training_loss: 0.007179077249020338\n",
      " epoch:4    training_loss: 0.009651285596191883\n",
      "testing_loss: 0.25628241896629333\n",
      "testing_loss: 0.22998586297035217\n",
      "testing_loss: 0.18508107960224152\n",
      "testing_loss: 0.2062755823135376\n",
      "testing_loss: 0.19750286638736725\n",
      "testing_loss: 0.24196875095367432\n",
      "testing_loss: 0.21150758862495422\n",
      "testing_loss: 0.22330264747142792\n",
      "testing_loss: 0.22713147103786469\n",
      "testing_loss: 0.26566123962402344\n",
      "testing_loss: 0.2026343196630478\n",
      "testing_loss: 0.24081940948963165\n",
      "testing_loss: 0.2328929752111435\n",
      "testing_loss: 0.2416536659002304\n",
      "testing_loss: 0.20650434494018555\n",
      "testing_loss: 0.18274857103824615\n",
      "testing_loss: 0.18417389690876007\n",
      "testing_loss: 0.23524689674377441\n",
      "testing_loss: 0.2502637505531311\n",
      "testing_loss: 0.19036978483200073\n",
      "testing_loss: 0.2338063269853592\n",
      "testing_loss: 0.24744157493114471\n",
      "testing_loss: 0.19091235101222992\n",
      "testing_loss: 0.2255186289548874\n",
      "testing_loss: 0.22485820949077606\n",
      "testing_loss: 0.22345948219299316\n",
      "testing_loss: 0.1791025549173355\n",
      "testing_loss: 0.17230276763439178\n",
      "testing_loss: 0.20269501209259033\n",
      "testing_loss: 0.1977447122335434\n",
      "testing_loss: 0.24113856256008148\n",
      "testing_loss: 0.2600536048412323\n",
      " epoch:4    testing_loss: 0.22633646428585052\n",
      "tensor([[0.2132],\n",
      "        [0.1706],\n",
      "        [0.1315],\n",
      "        [0.1140],\n",
      "        [0.6475],\n",
      "        [1.0000],\n",
      "        [0.9900],\n",
      "        [0.8118],\n",
      "        [0.6177],\n",
      "        [0.4758]], device='cuda:0') :::: tensor([[0.4842],\n",
      "        [0.4463],\n",
      "        [0.3779],\n",
      "        [0.3375],\n",
      "        [0.8057],\n",
      "        [1.0053],\n",
      "        [0.9221],\n",
      "        [0.7636],\n",
      "        [0.6267],\n",
      "        [0.5426]], device='cuda:0')\n",
      "tensor([[0.2863],\n",
      "        [0.2411],\n",
      "        [0.1896],\n",
      "        [0.1698],\n",
      "        [0.7299],\n",
      "        [1.0000],\n",
      "        [0.9310],\n",
      "        [0.7430],\n",
      "        [0.6203],\n",
      "        [0.8853]], device='cuda:0') :::: tensor([[0.1621],\n",
      "        [0.1714],\n",
      "        [0.1750],\n",
      "        [0.2169],\n",
      "        [0.7434],\n",
      "        [1.0009],\n",
      "        [0.9414],\n",
      "        [0.7963],\n",
      "        [0.6911],\n",
      "        [0.5943]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "## Params\n",
    "dim_val = 32\n",
    "n_heads = 8\n",
    "n_decoder_layers = 2\n",
    "n_encoder_layers = 2\n",
    "dec_seq_len = 10 # length of input given to decoder\n",
    "enc_seq_len = 10 # length of input given to encoder\n",
    "output_sequence_length = 10 # target sequence length. \n",
    "in_features_encoder_linear_layer = 128\n",
    "in_features_decoder_linear_layer = 128\n",
    "max_seq_len = enc_seq_len\n",
    "batch_first = True\n",
    "epochs = 5\n",
    "batch_size=64\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "                input_size=1,\n",
    "                dec_seq_len=enc_seq_len,\n",
    "                batch_first=batch_first\n",
    "                )\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Load the training data\n",
    "training_dataloader = DataLoader(training_data, batch_size= batch_size)\n",
    "# Load the testing data\n",
    "testing_dataloader = DataLoader(testing_data, batch_size= batch_size)\n",
    "\n",
    "# training prediction\n",
    "training_truple = ()\n",
    "# testing prediction\n",
    "testing_truple = ()\n",
    "\n",
    "# Iterate over all epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Iterate over all (x,y) pairs in training dataloader\n",
    "    for i, (src, trg, trg_y) in enumerate(training_dataloader):\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Permute from shape [batch size, seq len, num features] to [seq len, batch size, num features]\n",
    "        if batch_first == False:\n",
    "\n",
    "            shape_before = src.shape\n",
    "            src = src.permute(1, 0, 2)\n",
    "            print(\"src shape changed from {} to {}\".format(shape_before, src.shape))\n",
    "\n",
    "            shape_before = trg.shape\n",
    "            trg = trg.permute(1, 0, 2)\n",
    "            print(\"src shape changed from {} to {}\".format(shape_before, src.shape))\n",
    "\n",
    "\n",
    "        # Make src mask for decoder with size:\n",
    "        # [batch_size*n_heads, output_sequence_length, enc_seq_len]\n",
    "        src_mask = generate_square_subsequent_mask(\n",
    "            dim1=output_sequence_length,\n",
    "            dim2=enc_seq_len,\n",
    "            device = device \n",
    "            \n",
    "            )\n",
    "\n",
    "        # Make tgt mask for decoder with size:\n",
    "        # [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "        tgt_mask = generate_square_subsequent_mask( \n",
    "            dim1=output_sequence_length,\n",
    "            dim2=output_sequence_length,\n",
    "            device = device\n",
    "            )\n",
    "        # make prediction\n",
    "        prediction = model(\n",
    "            src=src,\n",
    "            tgt=trg,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "            )\n",
    "        # Compute and backprop loss\n",
    "        loss = criterion(trg_y, prediction)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Take optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i%100 == 0):\n",
    "            print(f'training_loss: {loss}')\n",
    "            \n",
    "        if (epoch == epochs-1):\n",
    "            training_truple += (prediction, )\n",
    "       \n",
    "    print(f' epoch:{epoch}    training_loss: {loss}')\n",
    "        \n",
    "    # Iterate over all (x,y) pairs in validation dataloader\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for j, (src, _, tgt_y) in enumerate(testing_dataloader):\n",
    "\n",
    "            prediction = run_encoder_decoder_inference(\n",
    "                model=model, \n",
    "                src=src, \n",
    "                batch_size=batch_size,\n",
    "                device = device,\n",
    "                output_sequence_length = output_sequence_length\n",
    "                )\n",
    "            loss = criterion(tgt_y, prediction)\n",
    "            \n",
    "            if (j%100 == 0):\n",
    "                print(f'testing_loss: {loss}')\n",
    "                \n",
    "            if (epoch == epochs-1):\n",
    "                testing_truple += (prediction, )\n",
    "                \n",
    "    print(f' epoch:{epoch}    testing_loss: {loss}')\n",
    "    print(tgt_y[0],\"::::\", prediction[0])\n",
    "    print(tgt_y[3],\"::::\", prediction[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0446ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_Sigdata = torch.cat(testing_truple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "038c76ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200000, 10, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_Sigdata.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cab4c0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(testing_Sigdata.size()[0]*0.6)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63fe71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_Sigdata = testing_Sigdata.detach()\n",
    "#training_Sigdata = training_Sigdata.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bda4ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_Sigdata1 = testing_Sigdata.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7592d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_Sigdata = testing_Sigdata1[size:]\n",
    "training_Sigdata = testing_Sigdata1[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb514e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80000, 10, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_Sigdata.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b9e1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for classifier models.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        data: torch.tensor,\n",
    "        label: torch.tensor\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "        print(\"data size = {}\".format(data.size()))\n",
    "        print(\"target size = {}\".format(label.size()))\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data.size()[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple with 3 elements:\n",
    "        1) src (the encoder input)\n",
    "        2) trg (the decoder input)\n",
    "        3) trg_y (the target)\n",
    "        \"\"\"\n",
    "        #print(self.data.size())\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5aefe13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size = torch.Size([120000, 10, 1])\n",
      "target size = torch.Size([120000, 1])\n"
     ]
    }
   ],
   "source": [
    "training_Sigclass = ClassifierDataset(\n",
    "    data = training_Sigdata ,\n",
    "    label = test_labels[:size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f913c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size = torch.Size([80000, 10, 1])\n",
      "target size = torch.Size([80000, 1])\n"
     ]
    }
   ],
   "source": [
    "testing_Sigclass = ClassifierDataset(\n",
    "    data = testing_Sigdata,\n",
    "    label = test_labels[size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e5d94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        input_size: int,\n",
    "        dec_seq_len: int,\n",
    "        batch_first: bool,\n",
    "        dim_val: int=256,  \n",
    "        n_encoder_layers: int=4,\n",
    "        n_heads: int=16,\n",
    "        dropout_encoder: float=0.2, \n",
    "        dropout_pos_enc: float=0.1,\n",
    "        dim_feedforward_encoder: int= 1024\n",
    "        ): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce \n",
    "                     outputs of dimension dim_val\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "            \n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "            \n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer \n",
    "                                     of the encoder\n",
    "            \n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    Most of the time, this will be 1 .\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=dim_val \n",
    "            )\n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=dim_val, \n",
    "            out_features=input_size\n",
    "            )\n",
    "        \n",
    "        self.final_linear_mapping = nn.Linear(\n",
    "            in_features=dec_seq_len, \n",
    "            out_features=input_size\n",
    "            )\n",
    "        \n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoder(\n",
    "            d_model=dim_val,\n",
    "            dropout=dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        # Creating the encoder layer \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerEncoder\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \n",
    "        Args:\n",
    "            src: the encoder's output sequence. Shape: \n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if \n",
    "                 batch_first=True, where S is the source sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    " \n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
    "        #print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "\n",
    "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "            )\n",
    "        #print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "        \n",
    "        # Pass through linear mapping\n",
    "        linear_output = self.linear_mapping(src) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): output size after linear_mapping = {}\".format(linear_output.size()))\n",
    "        \n",
    "        linear_output = linear_output.squeeze(-1)\n",
    "        #print(\"From model.forward(): output size after unsqeeze = {}\".format(linear_output.size()))\n",
    "        \n",
    "        # Pass through linear mapping\n",
    "        final_linear_output = self.final_linear_mapping(linear_output) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): output size after final linear_mapping = {}\".format(final_linear_output.size()))\n",
    "        \n",
    "        # Final output\n",
    "        final_output = self.output(final_linear_output)\n",
    "        #print(\"From model.forward(): final output size = {}\".format(final_output.size()))\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ab501e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.7001882791519165\n",
      "training_loss: 0.6750483512878418\n",
      "training_loss: 0.7011741399765015\n",
      "training_loss: 0.6591658592224121\n",
      "training_loss: 0.5030133724212646\n",
      "training_loss: 0.6767591834068298\n",
      "training_loss: 0.6132933497428894\n",
      "training_loss: 0.5241909027099609\n",
      "training_loss: 0.5480445027351379\n",
      "training_loss: 0.5968856811523438\n",
      "training_loss: 0.521196722984314\n",
      "training_loss: 0.5864609479904175\n",
      "training_loss: 0.5339520573616028\n",
      "training_loss: 0.4406255781650543\n",
      "training_loss: 0.5514427423477173\n",
      "training_loss: 0.5036472678184509\n",
      "training_loss: 0.6057994365692139\n",
      "training_loss: 0.5571581125259399\n",
      "training_loss: 0.5173980593681335\n",
      " epoch:0    training_loss: 0.5095445513725281\n",
      "testing_loss: 0.5943129062652588\n",
      "testing_loss: 0.542553186416626\n",
      "testing_loss: 0.6285238265991211\n",
      "testing_loss: 0.4898911714553833\n",
      "testing_loss: 0.5013012886047363\n",
      "testing_loss: 0.5773850679397583\n",
      "testing_loss: 0.7574921250343323\n",
      "testing_loss: 0.5399397611618042\n",
      "testing_loss: 0.5013620257377625\n",
      "testing_loss: 0.431423544883728\n",
      "testing_loss: 0.527830183506012\n",
      "testing_loss: 0.43778491020202637\n",
      "testing_loss: 0.5400000810623169\n",
      " epoch:0    testing_loss: 0.6022769808769226\n",
      "training_loss: 0.6158290505409241\n",
      "training_loss: 0.5340801477432251\n",
      "training_loss: 0.6119399070739746\n",
      "training_loss: 0.4106101989746094\n",
      "training_loss: 0.47012633085250854\n",
      "training_loss: 0.6510345935821533\n",
      "training_loss: 0.5918386578559875\n",
      "training_loss: 0.5331233739852905\n",
      "training_loss: 0.535618007183075\n",
      "training_loss: 0.5228524208068848\n",
      "training_loss: 0.5067408084869385\n",
      "training_loss: 0.5570020079612732\n",
      "training_loss: 0.4993102252483368\n",
      "training_loss: 0.45198649168014526\n",
      "training_loss: 0.5244364142417908\n",
      "training_loss: 0.5113778114318848\n",
      "training_loss: 0.6085361242294312\n",
      "training_loss: 0.5304950475692749\n",
      "training_loss: 0.48680561780929565\n",
      " epoch:1    training_loss: 0.5194300413131714\n",
      "testing_loss: 0.5827333927154541\n",
      "testing_loss: 0.5459662079811096\n",
      "testing_loss: 0.6137056350708008\n",
      "testing_loss: 0.4787237346172333\n",
      "testing_loss: 0.5013992786407471\n",
      "testing_loss: 0.5704688429832458\n",
      "testing_loss: 0.7133803367614746\n",
      "testing_loss: 0.5185848474502563\n",
      "testing_loss: 0.5253738164901733\n",
      "testing_loss: 0.4267992079257965\n",
      "testing_loss: 0.540534257888794\n",
      "testing_loss: 0.4408559203147888\n",
      "testing_loss: 0.5358225107192993\n",
      " epoch:1    testing_loss: 0.5975363254547119\n",
      "training_loss: 0.6223352551460266\n",
      "training_loss: 0.5242090821266174\n",
      "training_loss: 0.6039683818817139\n",
      "training_loss: 0.40005531907081604\n",
      "training_loss: 0.46811068058013916\n",
      "training_loss: 0.6567302942276001\n",
      "training_loss: 0.5883820652961731\n",
      "training_loss: 0.5289802551269531\n",
      "training_loss: 0.5404646992683411\n",
      "training_loss: 0.5238386392593384\n",
      "training_loss: 0.5084882974624634\n",
      "training_loss: 0.5533664226531982\n",
      "training_loss: 0.49672430753707886\n",
      "training_loss: 0.447887659072876\n",
      "training_loss: 0.5235450267791748\n",
      "training_loss: 0.5119571089744568\n",
      "training_loss: 0.6076837778091431\n",
      "training_loss: 0.5274165868759155\n",
      "training_loss: 0.4792855381965637\n",
      " epoch:2    training_loss: 0.5222564339637756\n",
      "testing_loss: 0.5916223526000977\n",
      "testing_loss: 0.5486968755722046\n",
      "testing_loss: 0.6106668710708618\n",
      "testing_loss: 0.4824851453304291\n",
      "testing_loss: 0.5026543140411377\n",
      "testing_loss: 0.5742765665054321\n",
      "testing_loss: 0.7106820344924927\n",
      "testing_loss: 0.5150469541549683\n",
      "testing_loss: 0.527595579624176\n",
      "testing_loss: 0.42583519220352173\n",
      "testing_loss: 0.5418683290481567\n",
      "testing_loss: 0.4373686909675598\n",
      "testing_loss: 0.5415441989898682\n",
      " epoch:2    testing_loss: 0.6040310263633728\n",
      "training_loss: 0.6201149225234985\n",
      "training_loss: 0.5239115357398987\n",
      "training_loss: 0.6002934575080872\n",
      "training_loss: 0.3940911889076233\n",
      "training_loss: 0.466991126537323\n",
      "training_loss: 0.6503137946128845\n",
      "training_loss: 0.5923396348953247\n",
      "training_loss: 0.5283071994781494\n",
      "training_loss: 0.5433682799339294\n",
      "training_loss: 0.5189290046691895\n",
      "training_loss: 0.5077617168426514\n",
      "training_loss: 0.5486147403717041\n",
      "training_loss: 0.4943355917930603\n",
      "training_loss: 0.44614219665527344\n",
      "training_loss: 0.5209687948226929\n",
      "training_loss: 0.5099688768386841\n",
      "training_loss: 0.6036636829376221\n",
      "training_loss: 0.5294853448867798\n",
      "training_loss: 0.4742123484611511\n",
      " epoch:3    training_loss: 0.5289308428764343\n",
      "testing_loss: 0.5834304690361023\n",
      "testing_loss: 0.5475813150405884\n",
      "testing_loss: 0.615384578704834\n",
      "testing_loss: 0.4750983417034149\n",
      "testing_loss: 0.4968045949935913\n",
      "testing_loss: 0.568867564201355\n",
      "testing_loss: 0.6950101852416992\n",
      "testing_loss: 0.501023530960083\n",
      "testing_loss: 0.5306637287139893\n",
      "testing_loss: 0.4165849983692169\n",
      "testing_loss: 0.5306409597396851\n",
      "testing_loss: 0.43012118339538574\n",
      "testing_loss: 0.5389073491096497\n",
      " epoch:3    testing_loss: 0.5959280729293823\n",
      "training_loss: 0.6209002733230591\n",
      "training_loss: 0.5282399654388428\n",
      "training_loss: 0.6026647090911865\n",
      "training_loss: 0.39983075857162476\n",
      "training_loss: 0.46264466643333435\n",
      "training_loss: 0.6484159231185913\n",
      "training_loss: 0.589529275894165\n",
      "training_loss: 0.5307321548461914\n",
      "training_loss: 0.5488189458847046\n",
      "training_loss: 0.5071951150894165\n",
      "training_loss: 0.5096256732940674\n",
      "training_loss: 0.54136061668396\n",
      "training_loss: 0.4905247986316681\n",
      "training_loss: 0.4398840665817261\n",
      "training_loss: 0.5114015936851501\n",
      "training_loss: 0.511615514755249\n",
      "training_loss: 0.5971356630325317\n",
      "training_loss: 0.5215749740600586\n",
      "training_loss: 0.4626067280769348\n",
      " epoch:4    training_loss: 0.5365519523620605\n",
      "testing_loss: 0.5824226140975952\n",
      "testing_loss: 0.5474488139152527\n",
      "testing_loss: 0.6174027919769287\n",
      "testing_loss: 0.48044708371162415\n",
      "testing_loss: 0.484133780002594\n",
      "testing_loss: 0.5647948384284973\n",
      "testing_loss: 0.6879212856292725\n",
      "testing_loss: 0.4835852384567261\n",
      "testing_loss: 0.5259745121002197\n",
      "testing_loss: 0.3977200388908386\n",
      "testing_loss: 0.5159115791320801\n",
      "testing_loss: 0.4254542887210846\n",
      "testing_loss: 0.539720892906189\n",
      " epoch:4    testing_loss: 0.5926055908203125\n"
     ]
    }
   ],
   "source": [
    "## Params\n",
    "dim_val = 256\n",
    "n_heads = 16\n",
    "n_encoder_layers = 4\n",
    "dec_seq_len = 10 # length of input given to decoder\n",
    "enc_seq_len = 10 # length of input given to encoder\n",
    "in_features_encoder_linear_layer = 256\n",
    "max_seq_len = enc_seq_len\n",
    "batch_first = True\n",
    "epochs = 5\n",
    "batch_size=64\n",
    "\n",
    "classifier_model = TimeSeriesClassifier(\n",
    "                input_size=1,\n",
    "                dec_seq_len=enc_seq_len,\n",
    "                batch_first=batch_first\n",
    "                )\n",
    "\n",
    "classifier_model = classifier_model.to(device)\n",
    "\n",
    "optimizer1 = torch.optim.Adam(classifier_model.parameters(), lr=0.0001)\n",
    "criterion1 = torch.nn.BCELoss()\n",
    "\n",
    "# Load the training data\n",
    "training_Sigdataloader = DataLoader(training_Sigclass, batch_size= batch_size)\n",
    "# Load the testing data\n",
    "testing_Sigdataloader = DataLoader(testing_Sigclass, batch_size= batch_size)\n",
    "\n",
    "# training prediction\n",
    "training_truple = ()\n",
    "training_truple_labels = ()\n",
    "# testing prediction\n",
    "testing_truple = ()\n",
    "testing_truple_labels = ()\n",
    "\n",
    "\n",
    "# Iterate over all epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Iterate over all (x,y) pairs in training dataloader\n",
    "    for i, (data, labels) in enumerate(training_Sigdataloader):\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer1.zero_grad()\n",
    "\n",
    "        # Permute from shape [batch size, seq len, num features] to [seq len, batch size, num features]\n",
    "        if batch_first == False:\n",
    "\n",
    "            shape_before = data.shape\n",
    "            data = data.permute(1, 0, 2)\n",
    "            print(\"src shape changed from {} to {}\".format(shape_before, data.shape))\n",
    "\n",
    "            shape_before = labels.shape\n",
    "            labels = labels.permute(1, 0)\n",
    "            print(\"labels shape changed from {} to {}\".format(shape_before, labels.shape))\n",
    "\n",
    "\n",
    "        # make prediction\n",
    "        prediction = classifier_model(\n",
    "            src=data\n",
    "            )\n",
    "        # Compute and backprop loss\n",
    "        loss = criterion1(prediction, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Take optimizer step\n",
    "        optimizer1.step()\n",
    "        \n",
    "        if (i%100 == 0):\n",
    "            print(f'training_loss: {loss}')\n",
    "        \n",
    "        if (epoch == epochs-1):\n",
    "                training_truple += (prediction, )\n",
    "                training_truple_labels +=(labels, )\n",
    "        \n",
    "    print(f' epoch:{epoch}    training_loss: {loss}')\n",
    "        \n",
    "    # Iterate over all (x,y) pairs in validation dataloader\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for j, (data_, labels_) in enumerate(testing_Sigdataloader):\n",
    "\n",
    "            prediction_ = classifier_model(\n",
    "                src=data_\n",
    "            )\n",
    "            loss_ = criterion1(prediction_, labels_)\n",
    "            \n",
    "            if (j%100 == 0):\n",
    "                print(f'testing_loss: {loss_}')\n",
    "            \n",
    "            if (epoch == epochs-1):\n",
    "                testing_truple += (prediction_, )\n",
    "                testing_truple_labels +=(labels_, )\n",
    "                \n",
    "    print(f' epoch:{epoch}    testing_loss: {loss_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03be0379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80000, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_hat = torch.cat(testing_truple)\n",
    "testing_hat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e9719ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80000, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_org = torch.cat(testing_truple_labels)\n",
    "testing_org.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51ba434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = testing_org.data.cpu().numpy()\n",
    "predictions = testing_hat.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc075716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5353a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test labels are: [1. 1. 0. ... 1. 1. 0.]\n",
      "[[0.16582887]\n",
      " [0.4159264 ]\n",
      " [0.33749297]\n",
      " ...\n",
      " [0.754509  ]\n",
      " [0.08159296]\n",
      " [0.76210606]]\n",
      "[[0.7556625 ]\n",
      " [0.7578976 ]\n",
      " [0.07953998]\n",
      " ...\n",
      " [0.71980006]\n",
      " [0.11855207]\n",
      " [0.756114  ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'test labels are: {y_test[:,0]}')\n",
    "\n",
    "# Create separate arrays for background and signal predictions\n",
    "bkg_preds = predictions[y_test[:,0] == 0]\n",
    "sig_preds = predictions[y_test[:,0] == 1]\n",
    "\n",
    "print(bkg_preds)\n",
    "print(sig_preds)\n",
    "# Plot the predicted probabilities\n",
    "plt.hist(bkg_preds, bins=50, label='Background', alpha=0.5)\n",
    "plt.hist(sig_preds, bins=50, label='Signal', alpha=0.5, color='green')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.legend(loc='upper center')\n",
    "#plt.ylim(top=100)\n",
    "#plt.show()\n",
    "plt.savefig(\"bkgVsSig_v2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a86646d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.xlim([0.9, 1])\n",
    "#plt.ylim([0.999, 1.005])\n",
    "#plt.show()\n",
    "plt.savefig(\"ROC_v2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e852f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### precision recall curve\n",
    "# Calculate precision-recall curve and average precision\n",
    "precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "average_precision = average_precision_score(y_test, predictions)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "172a2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_prediction = predictions > 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b8d8590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59858"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(binary_prediction == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce4c7c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601ec6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
