{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad4fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "import uproot\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn \n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144f4a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06895913  0.1056133  -0.09143379 ... -0.42161528 -0.29712962\n",
      "  -0.58216445]\n",
      " [ 0.34485138 -0.08731802  0.06661344 ...  0.77302449  0.04550623\n",
      "   0.00307311]\n",
      " [ 0.1237994   0.15758994  0.23702709 ...  0.74946809  0.89637208\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.03092482  0.05597267  0.02970392 ...  0.63602553  0.47977004\n",
      "   0.197776  ]\n",
      " [-0.07213616  0.50158804  0.37323287 ...  0.94731449  1.\n",
      "   0.95559548]\n",
      " [ 0.093267   -0.1169253  -0.21280756 ...  0.9604724   1.\n",
      "   0.49827842]]\n",
      "Printing y_real\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "### Energy = 0.15 GeV\n",
    "root_file = uproot.open(f\"/eos/user/d/dasgupsu/SWAN_projects/ECAL_noise_EM_discrimination/data/outputPSWithPU_withNoise_0.150000_0.15_v11.root\")\n",
    "tree = root_file[\"Samples\"]\n",
    "arrays = tree.arrays([\"samples\", \"ysamples\",\"samplesNoise\",\"ysamplesNoise\", \"waveform\"])\n",
    "X_real = ak.to_numpy(arrays[\"samples\"])\n",
    "y_real = ak.to_numpy(arrays[\"ysamples\"])\n",
    "X_noise = ak.to_numpy(arrays[\"samplesNoise\"])\n",
    "y_noise = ak.to_numpy(arrays[\"ysamplesNoise\"])\n",
    "X_waveform = ak.to_numpy(arrays[\"waveform\"])\n",
    "X_Zero = np.zeros((X_waveform.shape[0], X_waveform.shape[1]), dtype=float)\n",
    "\n",
    "print(X_real) \n",
    "print(\"Printing y_real\")\n",
    "print(y_real)\n",
    "\n",
    "data = np.concatenate([X_real, X_noise]) ### makes it [2*num_events,num_samples]\n",
    "#data = X_real\n",
    "labels = np.concatenate([y_real,y_noise])\n",
    "#labels = y_real\n",
    "X_target = np.concatenate([X_waveform, X_Zero])\n",
    "#X_target = X_waveform\n",
    "# Shuffle data and labels together\n",
    "## Important to shuffle since I take some fraction of events so it should not happen that all the real events \n",
    "## are cluttered at the beginning\n",
    "\n",
    "shuffle_indices = np.random.permutation(len(data))\n",
    "data = data[shuffle_indices]\n",
    "labels = labels[shuffle_indices]\n",
    "X_target = X_target[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decee919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in data is 400000\n",
      "number of rows in data is 10\n",
      "Size of training data is 200000\n",
      "number of elements in data : training data : test data : test target : train target: 400000 : 200000 : 200000: 200000: 200000\n"
     ]
    }
   ],
   "source": [
    "import awkward as ak\n",
    "\n",
    "num_events_data = ak.num(data, axis=0)\n",
    "print(f'number of rows in data is {num_events_data}')\n",
    "\n",
    "\n",
    "ntimeSamples_data = ak.num(data, axis=1)\n",
    "print(f'number of rows in data is {ntimeSamples_data[0]}') ## just take the 0th event\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(0.5 * num_events_data) ###times 2 because the noise is also in the same dataset, so it is 2*num_events\n",
    "train_data = data[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "train_target = X_target[:train_size]\n",
    "test_data = data[train_size:]\n",
    "test_labels = labels[train_size:]\n",
    "test_target = X_target[train_size:]\n",
    "print(f'Size of training data is {train_size}')\n",
    "\n",
    "'''\n",
    "print(train_size)\n",
    "print(train_data)\n",
    "print(test_data)\n",
    "'''\n",
    "\n",
    "print(f'number of elements in data : training data : test data : test target : train target: {len(data)} : {len(train_data)} : {len(test_data)}: {len(test_target)}: {len(train_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c747d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reshape((train_data.shape[0], train_data.shape[1], 1))\n",
    "train_target = train_target.reshape((train_target.shape[0], train_target.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbba1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.reshape((test_data.shape[0], test_data.shape[1], 1))\n",
    "test_target = test_target.reshape((test_target.shape[0], test_target.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0c4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.reshape((train_labels.shape[0], 1))\n",
    "test_labels = test_labels.reshape((test_labels.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6b2296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be65f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_labels, dtype =torch.float).to(device)\n",
    "test_labels = torch.tensor(test_labels, dtype =torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4956d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_target[59904]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f21ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data, dtype =torch.float).to(device)\n",
    "train_target = torch.tensor(train_target, dtype =torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c97d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.tensor(test_data, dtype =torch.float).to(device)\n",
    "test_target = torch.tensor(test_target, dtype =torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc00706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200000, 10, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be49af7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200000, 10, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71531c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for transformer models.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        data: torch.tensor,\n",
    "        target: torch.tensor,\n",
    "        device\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.device = device\n",
    "\n",
    "        print(\"data size = {}\".format(data.size()))\n",
    "        print(\"target size = {}\".format(target.size()))\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data.size()[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple with 3 elements:\n",
    "        1) src (the encoder input)\n",
    "        2) trg (the decoder input)\n",
    "        3) trg_y (the target)\n",
    "        \"\"\"\n",
    "        #print(self.data.size())\n",
    "        src = self.data[index]\n",
    "        \n",
    "        start_token = 99.*torch.ones((self.target.size()[0],1,1)).to(self.device)\n",
    "        target = torch.cat((start_token, self.target),1)[index]\n",
    "        \n",
    "        trg = target[:-1,:]\n",
    "        trg_y = target[1:,:]\n",
    "        \n",
    "\n",
    "        return src, trg, trg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cce949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size = torch.Size([200000, 10, 1])\n",
      "target size = torch.Size([200000, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "training_data = TransformerDataset(\n",
    "    data = train_data,\n",
    "    target = train_target,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "802684e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size = torch.Size([200000, 10, 1])\n",
      "target size = torch.Size([200000, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "testing_data = TransformerDataset(\n",
    "    data = test_data,\n",
    "    target = test_target,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ced4fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dropout: float=0.1, \n",
    "        max_seq_len: int=10, \n",
    "        d_model: int=32,\n",
    "        batch_first: bool=True\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # adapted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        if self.batch_first:\n",
    "            pe = torch.zeros(1, max_seq_len, d_model)\n",
    "            \n",
    "            pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "            \n",
    "            pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        \n",
    "            pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "            pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or \n",
    "               [enc_seq_len, batch_size, dim_val]\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            x = x + self.pe[:,:x.size(1)]\n",
    "        else:\n",
    "            x = x + self.pe[:x.size(0)]\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1db402a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(dim1: int, dim2: int, device) -> Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "        dim1: int,  target sequence length\n",
    "\n",
    "        dim2: int, for src masking this must be encoder sequence length (i.e. \n",
    "              the length of the input sequence to the model), \n",
    "              and for tgt masking, this must be target sequence length \n",
    "\n",
    "\n",
    "    Return:\n",
    "\n",
    "        A Tensor of shape [dim1, dim2]\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "430600dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        input_size: int,\n",
    "        dec_seq_len: int,\n",
    "        batch_first: bool,\n",
    "        out_seq_len: int=10,\n",
    "        dim_val: int=32,  \n",
    "        n_encoder_layers: int=2,\n",
    "        n_decoder_layers: int=2,\n",
    "        n_heads: int=8,\n",
    "        dropout_encoder: float=0.1, \n",
    "        dropout_decoder: float=0.1,\n",
    "        dropout_pos_enc: float=0.1,\n",
    "        dim_feedforward_encoder: int=128,\n",
    "        dim_feedforward_decoder: int=128\n",
    "        ): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce \n",
    "                     outputs of dimension dim_val\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "            dropout_decoder: float, the dropout rate of the decoder\n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer \n",
    "                                     of the encoder\n",
    "            dim_feedforward_decoder: int, number of neurons in the linear layer \n",
    "                                     of the decoder\n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    Most of the time, this will be 1 .\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=dim_val \n",
    "            )\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=input_size,\n",
    "            out_features=dim_val\n",
    "            )  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=dim_val, \n",
    "            out_features=input_size\n",
    "            )\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoder(\n",
    "            d_model=dim_val,\n",
    "            dropout=dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        # Creating the encoder layer \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerEncoder\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the decoder layers in nn.TransformerDecoder\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, \n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \n",
    "        Args:\n",
    "            src: the encoder's output sequence. Shape: \n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if \n",
    "                 batch_first=True, where S is the source sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            tgt: the sequence to the decoder. Shape: \n",
    "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if \n",
    "                 batch_first=True, where T is the target sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            src_mask: the mask for the src sequence to prevent the model from \n",
    "                      using data points from the target sequence\n",
    "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
    "                      using data points from the target sequence\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
    "        #print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "\n",
    "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "            )\n",
    "        #print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "\n",
    "        # Pass decoder input through decoder input layer\n",
    "        decoder_output = self.decoder_input_layer(tgt) # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\n",
    "\n",
    "        #if src_mask is not None:\n",
    "            #print(\"From model.forward(): Size of src_mask: {}\".format(src_mask.size()))\n",
    "        #if tgt_mask is not None:\n",
    "            #print(\"From model.forward(): Size of tgt_mask: {}\".format(tgt_mask.size()))\n",
    "\n",
    "        # Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "\n",
    "        #print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\n",
    "\n",
    "        # Pass through linear mapping\n",
    "        decoder_output = self.linear_mapping(decoder_output) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): decoder_output size after linear_mapping = {}\".format(decoder_output.size()))\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd189d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_encoder_decoder_inference(\n",
    "    model: nn.Module, \n",
    "    src: torch.Tensor, \n",
    "    batch_size: int,\n",
    "    device,\n",
    "    output_sequence_length: int,\n",
    "    batch_first: bool=True\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        model: An encoder-decoder type model where the decoder requires\n",
    "               target values as input. Should be set to evaluation mode before \n",
    "               passed to this function.\n",
    "               \n",
    "        src: The input to the model\n",
    "        \n",
    "        output_sequence_length: The desired length of the model's output\n",
    "        \n",
    "        batch_size: batch size\n",
    "        \n",
    "        batch_first: If true, the shape of the model input should be \n",
    "                     [batch size, input sequence length, number of features].\n",
    "                     If false, [input sequence length, batch size, number of features]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Dimension of a batched model input that contains the target sequence values\n",
    "    target_seq_dim = 0 if batch_first == False else 1\n",
    "\n",
    "    # Take the last value of thetarget variable in all batches in src and make it tgt\n",
    "    # as per the Influenza paper\n",
    "    tgt = 99.0*torch.ones(1, batch_size, 1).to(device) if batch_first == False else 99.0*torch.ones(batch_size, 1, 1).to(device) # shape [1, batch_size, 1]\n",
    "\n",
    "    # Iteratively concatenate tgt with the first element in the prediction\n",
    "    for _ in range(output_sequence_length-1):\n",
    "\n",
    "        # Create masks\n",
    "        dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "        dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_a,\n",
    "            device = device\n",
    "            )\n",
    "\n",
    "        src_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_b,\n",
    "            device = device\n",
    "            )\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model(src, tgt, src_mask, tgt_mask) \n",
    "\n",
    "        # If statement simply makes sure that the predicted value is \n",
    "        # extracted and reshaped correctly\n",
    "        if batch_first == False:\n",
    "\n",
    "            # Obtain the predicted value at t+1 where t is the last time step \n",
    "            # represented in tgt\n",
    "            last_predicted_value = prediction[-1, :, :] \n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [1, batch_size, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Obtain predicted value\n",
    "            last_predicted_value = prediction[:, -1, :]\n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [batch_size, 1, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(-1)\n",
    "\n",
    "        # Detach the predicted element from the graph and concatenate with \n",
    "        # tgt in dimension 1 or 0\n",
    "        tgt = torch.cat((tgt, last_predicted_value.detach()), target_seq_dim)\n",
    "    \n",
    "    # Create masks\n",
    "    dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "    dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_a,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_b,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    # Make final prediction\n",
    "    final_prediction = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f71d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.4890819489955902\n",
      "training_loss: 0.030118584632873535\n",
      "training_loss: 0.019553570076823235\n",
      "training_loss: 0.019089654088020325\n",
      "training_loss: 0.015393859706819057\n",
      "training_loss: 0.012627785094082355\n",
      "training_loss: 0.009152339771389961\n",
      "training_loss: 0.013641664758324623\n",
      "training_loss: 0.014163061045110226\n",
      "training_loss: 0.013746603392064571\n",
      "training_loss: 0.012685599736869335\n",
      "training_loss: 0.009465138427913189\n",
      "training_loss: 0.011269405484199524\n",
      "training_loss: 0.015644369646906853\n",
      "training_loss: 0.01585087738931179\n",
      "training_loss: 0.00752361211925745\n",
      "training_loss: 0.01016672421246767\n",
      "training_loss: 0.011667094193398952\n",
      "training_loss: 0.009286485612392426\n",
      "training_loss: 0.00913168489933014\n",
      "training_loss: 0.009491942822933197\n",
      "training_loss: 0.009176907129585743\n",
      "training_loss: 0.010550717823207378\n",
      "training_loss: 0.012677221558988094\n",
      "training_loss: 0.009128576144576073\n",
      "training_loss: 0.009969622828066349\n",
      "training_loss: 0.00601743021979928\n",
      "training_loss: 0.007458026986569166\n",
      "training_loss: 0.009851058013737202\n",
      "training_loss: 0.008094721473753452\n",
      "training_loss: 0.006200878415256739\n",
      "training_loss: 0.0074149430729448795\n",
      " epoch:0    training_loss: 0.007126379758119583\n",
      "testing_loss: 0.23740997910499573\n",
      "testing_loss: 0.22099004685878754\n",
      "testing_loss: 0.1829526424407959\n",
      "testing_loss: 0.18656793236732483\n",
      "testing_loss: 0.21730934083461761\n",
      "testing_loss: 0.15608477592468262\n",
      "testing_loss: 0.19619329273700714\n",
      "testing_loss: 0.1547856479883194\n",
      "testing_loss: 0.2069101631641388\n",
      "testing_loss: 0.19033363461494446\n",
      "testing_loss: 0.17894434928894043\n",
      "testing_loss: 0.17196886241436005\n",
      "testing_loss: 0.1541299819946289\n",
      "testing_loss: 0.1690196841955185\n",
      "testing_loss: 0.1594397872686386\n",
      "testing_loss: 0.1782093048095703\n",
      "testing_loss: 0.1723828762769699\n",
      "testing_loss: 0.18253012001514435\n",
      "testing_loss: 0.17657311260700226\n",
      "testing_loss: 0.19974152743816376\n",
      "testing_loss: 0.13385391235351562\n",
      "testing_loss: 0.18188540637493134\n",
      "testing_loss: 0.17973405122756958\n",
      "testing_loss: 0.15418380498886108\n",
      "testing_loss: 0.16374395787715912\n",
      "testing_loss: 0.17402516305446625\n",
      "testing_loss: 0.19799385964870453\n",
      "testing_loss: 0.2261582463979721\n",
      "testing_loss: 0.2079412043094635\n",
      "testing_loss: 0.16936102509498596\n",
      "testing_loss: 0.18765351176261902\n",
      "testing_loss: 0.15041761100292206\n",
      " epoch:0    testing_loss: 0.19154436886310577\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.1044],\n",
      "        [0.0905],\n",
      "        [0.1048],\n",
      "        [0.1403],\n",
      "        [0.7123],\n",
      "        [0.9991],\n",
      "        [0.9353],\n",
      "        [0.7753],\n",
      "        [0.6283],\n",
      "        [0.4883]], device='cuda:0')\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.4161],\n",
      "        [0.4218],\n",
      "        [0.4056],\n",
      "        [0.3721],\n",
      "        [0.8217],\n",
      "        [0.9991],\n",
      "        [0.9109],\n",
      "        [0.7438],\n",
      "        [0.5853],\n",
      "        [0.4465]], device='cuda:0')\n",
      "training_loss: 0.00926586240530014\n",
      "training_loss: 0.007839393801987171\n",
      "training_loss: 0.007925164885818958\n",
      "training_loss: 0.00816079881042242\n",
      "training_loss: 0.00791945867240429\n",
      "training_loss: 0.006540477275848389\n",
      "training_loss: 0.004026449751108885\n",
      "training_loss: 0.008212628774344921\n",
      "training_loss: 0.007316209375858307\n",
      "training_loss: 0.007366485893726349\n",
      "training_loss: 0.009011621586978436\n",
      "training_loss: 0.005782932974398136\n",
      "training_loss: 0.006596476770937443\n",
      "training_loss: 0.012418925762176514\n",
      "training_loss: 0.01235544215887785\n",
      "training_loss: 0.004954357631504536\n",
      "training_loss: 0.0070907860063016415\n",
      "training_loss: 0.006921636406332254\n",
      "training_loss: 0.006654179189354181\n",
      "training_loss: 0.007973450236022472\n",
      "training_loss: 0.007309646345674992\n",
      "training_loss: 0.006215197965502739\n",
      "training_loss: 0.0066308677196502686\n",
      "training_loss: 0.008623971603810787\n",
      "training_loss: 0.00708742206916213\n",
      "training_loss: 0.006207396276295185\n",
      "training_loss: 0.004835605155676603\n",
      "training_loss: 0.005002155434340239\n",
      "training_loss: 0.006119038909673691\n",
      "training_loss: 0.005875496193766594\n",
      "training_loss: 0.004105794709175825\n",
      "training_loss: 0.0049912333488464355\n",
      " epoch:1    training_loss: 0.00591554120182991\n",
      "testing_loss: 0.27042174339294434\n",
      "testing_loss: 0.25378498435020447\n",
      "testing_loss: 0.20553350448608398\n",
      "testing_loss: 0.2209116518497467\n",
      "testing_loss: 0.2792283892631531\n",
      "testing_loss: 0.1950702965259552\n",
      "testing_loss: 0.22399461269378662\n",
      "testing_loss: 0.18360595405101776\n",
      "testing_loss: 0.2480987161397934\n",
      "testing_loss: 0.21912722289562225\n",
      "testing_loss: 0.20745602250099182\n",
      "testing_loss: 0.19976699352264404\n",
      "testing_loss: 0.1957758516073227\n",
      "testing_loss: 0.19622857868671417\n",
      "testing_loss: 0.17618803679943085\n",
      "testing_loss: 0.2065914124250412\n",
      "testing_loss: 0.21248295903205872\n",
      "testing_loss: 0.22082172334194183\n",
      "testing_loss: 0.20859988033771515\n",
      "testing_loss: 0.21914182603359222\n",
      "testing_loss: 0.1692676991224289\n",
      "testing_loss: 0.2206166833639145\n",
      "testing_loss: 0.22137223184108734\n",
      "testing_loss: 0.1933794468641281\n",
      "testing_loss: 0.19288651645183563\n",
      "testing_loss: 0.2281350940465927\n",
      "testing_loss: 0.22792772948741913\n",
      "testing_loss: 0.26306384801864624\n",
      "testing_loss: 0.23466196656227112\n",
      "testing_loss: 0.20327840745449066\n",
      "testing_loss: 0.23615601658821106\n",
      "testing_loss: 0.17417749762535095\n",
      " epoch:1    testing_loss: 0.22673960030078888\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.0915],\n",
      "        [0.1246],\n",
      "        [0.1738],\n",
      "        [0.2489],\n",
      "        [0.7762],\n",
      "        [1.0119],\n",
      "        [0.9211],\n",
      "        [0.7551],\n",
      "        [0.5998],\n",
      "        [0.4880]], device='cuda:0')\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.4462],\n",
      "        [0.4743],\n",
      "        [0.4629],\n",
      "        [0.4715],\n",
      "        [0.8754],\n",
      "        [1.0202],\n",
      "        [0.8991],\n",
      "        [0.7449],\n",
      "        [0.6065],\n",
      "        [0.4983]], device='cuda:0')\n",
      "training_loss: 0.008369897492229939\n",
      "training_loss: 0.007003192789852619\n",
      "training_loss: 0.0073407976888120174\n",
      "training_loss: 0.008216786198318005\n",
      "training_loss: 0.007271555718034506\n",
      "training_loss: 0.006409409921616316\n",
      "training_loss: 0.0036854275967925787\n",
      "training_loss: 0.007734441664069891\n",
      "training_loss: 0.006771209184080362\n",
      "training_loss: 0.006481282413005829\n",
      "training_loss: 0.008809980005025864\n",
      "training_loss: 0.005301293917000294\n",
      "training_loss: 0.0061553530395030975\n",
      "training_loss: 0.011559429578483105\n",
      "training_loss: 0.012076334096491337\n",
      "training_loss: 0.004599892999976873\n",
      "training_loss: 0.006867130286991596\n",
      "training_loss: 0.006583977025002241\n",
      "training_loss: 0.006455094553530216\n",
      "training_loss: 0.007133200764656067\n",
      "training_loss: 0.006787354592233896\n",
      "training_loss: 0.008847932331264019\n",
      "training_loss: 0.00662937993183732\n",
      "training_loss: 0.008777505718171597\n",
      "training_loss: 0.006533380597829819\n",
      "training_loss: 0.006036014761775732\n",
      "training_loss: 0.004567336291074753\n",
      "training_loss: 0.0048391567543148994\n",
      "training_loss: 0.005837596021592617\n",
      "training_loss: 0.005588031839579344\n",
      "training_loss: 0.004031164105981588\n",
      "training_loss: 0.005104999523609877\n",
      " epoch:2    training_loss: 0.005944723729044199\n",
      "testing_loss: 0.26433730125427246\n",
      "testing_loss: 0.24718089401721954\n",
      "testing_loss: 0.2044178694486618\n",
      "testing_loss: 0.21924947202205658\n",
      "testing_loss: 0.269637793302536\n",
      "testing_loss: 0.191533625125885\n",
      "testing_loss: 0.22335277497768402\n",
      "testing_loss: 0.18560171127319336\n",
      "testing_loss: 0.24059708416461945\n",
      "testing_loss: 0.21780462563037872\n",
      "testing_loss: 0.20336151123046875\n",
      "testing_loss: 0.2011374980211258\n",
      "testing_loss: 0.18930931389331818\n",
      "testing_loss: 0.18956592679023743\n",
      "testing_loss: 0.17776842415332794\n",
      "testing_loss: 0.19949233531951904\n",
      "testing_loss: 0.20759446918964386\n",
      "testing_loss: 0.2135302871465683\n",
      "testing_loss: 0.2016666978597641\n",
      "testing_loss: 0.2104501724243164\n",
      "testing_loss: 0.1673639416694641\n",
      "testing_loss: 0.21487799286842346\n",
      "testing_loss: 0.2153875082731247\n",
      "testing_loss: 0.1941213756799698\n",
      "testing_loss: 0.19628451764583588\n",
      "testing_loss: 0.22278335690498352\n",
      "testing_loss: 0.22195349633693695\n",
      "testing_loss: 0.2555302679538727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_loss: 0.2358996868133545\n",
      "testing_loss: 0.21103444695472717\n",
      "testing_loss: 0.23395541310310364\n",
      "testing_loss: 0.17948073148727417\n",
      " epoch:2    testing_loss: 0.22237420082092285\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.0820],\n",
      "        [0.0901],\n",
      "        [0.1084],\n",
      "        [0.1746],\n",
      "        [0.7258],\n",
      "        [1.0013],\n",
      "        [0.9446],\n",
      "        [0.7997],\n",
      "        [0.6318],\n",
      "        [0.4739]], device='cuda:0')\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.4200],\n",
      "        [0.4358],\n",
      "        [0.4374],\n",
      "        [0.4792],\n",
      "        [0.8726],\n",
      "        [0.9974],\n",
      "        [0.9136],\n",
      "        [0.7523],\n",
      "        [0.5870],\n",
      "        [0.4514]], device='cuda:0')\n",
      "training_loss: 0.00833536870777607\n",
      "training_loss: 0.00698666600510478\n",
      "training_loss: 0.007220140192657709\n",
      "training_loss: 0.007806453388184309\n",
      "training_loss: 0.007368846330791712\n",
      "training_loss: 0.006071288138628006\n",
      "training_loss: 0.0033668726682662964\n",
      "training_loss: 0.007956977002322674\n",
      "training_loss: 0.006712456699460745\n",
      "training_loss: 0.006414740812033415\n",
      "training_loss: 0.008849614299833775\n",
      "training_loss: 0.006753562483936548\n",
      "training_loss: 0.006385364569723606\n",
      "training_loss: 0.011288857087492943\n",
      "training_loss: 0.012267769314348698\n",
      "training_loss: 0.00444039236754179\n",
      "training_loss: 0.006912729237228632\n",
      "training_loss: 0.006604024209082127\n",
      "training_loss: 0.006169149652123451\n",
      "training_loss: 0.006916384678333998\n",
      "training_loss: 0.006706164218485355\n",
      "training_loss: 0.006152930669486523\n",
      "training_loss: 0.006571733858436346\n",
      "training_loss: 0.008519037626683712\n",
      "training_loss: 0.006585503462702036\n",
      "training_loss: 0.0059533254243433475\n",
      "training_loss: 0.004397624172270298\n",
      "training_loss: 0.004578189458698034\n",
      "training_loss: 0.005906566511839628\n",
      "training_loss: 0.005302866455167532\n",
      "training_loss: 0.0040946477092802525\n",
      "training_loss: 0.004643817897886038\n",
      " epoch:3    training_loss: 0.006970082875341177\n",
      "testing_loss: 0.2588779032230377\n",
      "testing_loss: 0.23111462593078613\n",
      "testing_loss: 0.20286498963832855\n",
      "testing_loss: 0.19763605296611786\n",
      "testing_loss: 0.2506782114505768\n",
      "testing_loss: 0.15709280967712402\n",
      "testing_loss: 0.21895161271095276\n",
      "testing_loss: 0.18232661485671997\n",
      "testing_loss: 0.22411327064037323\n",
      "testing_loss: 0.21219420433044434\n",
      "testing_loss: 0.20421233773231506\n",
      "testing_loss: 0.2060288041830063\n",
      "testing_loss: 0.1774425059556961\n",
      "testing_loss: 0.18864771723747253\n",
      "testing_loss: 0.17050696909427643\n",
      "testing_loss: 0.1987905353307724\n",
      "testing_loss: 0.1936328262090683\n",
      "testing_loss: 0.2036791294813156\n",
      "testing_loss: 0.184899240732193\n",
      "testing_loss: 0.21640074253082275\n",
      "testing_loss: 0.15651151537895203\n",
      "testing_loss: 0.21060171723365784\n",
      "testing_loss: 0.17763102054595947\n",
      "testing_loss: 0.16482341289520264\n",
      "testing_loss: 0.1809687465429306\n",
      "testing_loss: 0.20149138569831848\n",
      "testing_loss: 0.2116556465625763\n",
      "testing_loss: 0.2555871903896332\n",
      "testing_loss: 0.22347266972064972\n",
      "testing_loss: 0.19037321209907532\n",
      "testing_loss: 0.20432639122009277\n",
      "testing_loss: 0.16865582764148712\n",
      " epoch:3    testing_loss: 0.2131887525320053\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.0674],\n",
      "        [0.0763],\n",
      "        [0.0956],\n",
      "        [0.1765],\n",
      "        [0.7393],\n",
      "        [0.9820],\n",
      "        [0.9234],\n",
      "        [0.8008],\n",
      "        [0.7064],\n",
      "        [0.5996]], device='cuda:0')\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.4636],\n",
      "        [0.4548],\n",
      "        [0.4294],\n",
      "        [0.4613],\n",
      "        [0.8627],\n",
      "        [0.9878],\n",
      "        [0.8950],\n",
      "        [0.7746],\n",
      "        [0.6942],\n",
      "        [0.6190]], device='cuda:0')\n",
      "training_loss: 0.01005823165178299\n",
      "training_loss: 0.007142108865082264\n",
      "training_loss: 0.006921357940882444\n",
      "training_loss: 0.007323498371988535\n",
      "training_loss: 0.007208400871604681\n",
      "training_loss: 0.006182752083986998\n",
      "training_loss: 0.0033921280410140753\n",
      "training_loss: 0.007471208460628986\n",
      "training_loss: 0.0064958143047988415\n",
      "training_loss: 0.006164352875202894\n",
      "training_loss: 0.00882770586758852\n",
      "training_loss: 0.006533259991556406\n",
      "training_loss: 0.005915153305977583\n",
      "training_loss: 0.011552730575203896\n",
      "training_loss: 0.012167668901383877\n",
      "training_loss: 0.004472981672734022\n",
      "training_loss: 0.0069235945120453835\n",
      "training_loss: 0.00625689048320055\n",
      "training_loss: 0.006288984324783087\n",
      "training_loss: 0.00689543504267931\n",
      "training_loss: 0.006883311551064253\n",
      "training_loss: 0.006305878050625324\n",
      "training_loss: 0.006784808821976185\n",
      "training_loss: 0.00842890702188015\n",
      "training_loss: 0.006321884226053953\n",
      "training_loss: 0.006654040422290564\n",
      "training_loss: 0.004664145410060883\n",
      "training_loss: 0.004839039873331785\n",
      "training_loss: 0.00586591986939311\n",
      "training_loss: 0.004940294660627842\n",
      "training_loss: 0.00398244708776474\n",
      "training_loss: 0.004418433643877506\n",
      " epoch:4    training_loss: 0.005759228020906448\n",
      "testing_loss: 0.2672749161720276\n",
      "testing_loss: 0.2380528301000595\n",
      "testing_loss: 0.2048596888780594\n",
      "testing_loss: 0.2088393270969391\n",
      "testing_loss: 0.258327454328537\n",
      "testing_loss: 0.17423765361309052\n",
      "testing_loss: 0.21060311794281006\n",
      "testing_loss: 0.19093208014965057\n",
      "testing_loss: 0.23391222953796387\n",
      "testing_loss: 0.22927360236644745\n",
      "testing_loss: 0.20695506036281586\n",
      "testing_loss: 0.2060914784669876\n",
      "testing_loss: 0.1841425597667694\n",
      "testing_loss: 0.18964174389839172\n",
      "testing_loss: 0.1832767277956009\n",
      "testing_loss: 0.2020910531282425\n",
      "testing_loss: 0.20246534049510956\n",
      "testing_loss: 0.2242203801870346\n",
      "testing_loss: 0.20123004913330078\n",
      "testing_loss: 0.222262904047966\n",
      "testing_loss: 0.16456307470798492\n",
      "testing_loss: 0.22150535881519318\n",
      "testing_loss: 0.1991928368806839\n",
      "testing_loss: 0.17968355119228363\n",
      "testing_loss: 0.1882561445236206\n",
      "testing_loss: 0.23072068393230438\n",
      "testing_loss: 0.2254655808210373\n",
      "testing_loss: 0.25918886065483093\n",
      "testing_loss: 0.23672893643379211\n",
      "testing_loss: 0.1762435883283615\n",
      "testing_loss: 0.22159264981746674\n",
      "testing_loss: 0.1700456738471985\n",
      " epoch:4    testing_loss: 0.23541417717933655\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.0577],\n",
      "        [0.0634],\n",
      "        [0.0851],\n",
      "        [0.1512],\n",
      "        [0.7130],\n",
      "        [0.9870],\n",
      "        [0.9724],\n",
      "        [0.9077],\n",
      "        [0.8395],\n",
      "        [0.8115]], device='cuda:0')\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0') :::: tensor([[0.5291],\n",
      "        [0.5128],\n",
      "        [0.4591],\n",
      "        [0.4309],\n",
      "        [0.8368],\n",
      "        [0.9967],\n",
      "        [0.8941],\n",
      "        [0.7307],\n",
      "        [0.5797],\n",
      "        [0.4665]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "## Params\n",
    "dim_val = 32\n",
    "n_heads = 8\n",
    "n_decoder_layers = 2\n",
    "n_encoder_layers = 2\n",
    "dec_seq_len = 10 # length of input given to decoder\n",
    "enc_seq_len = 10 # length of input given to encoder\n",
    "output_sequence_length = 10 # target sequence length. \n",
    "in_features_encoder_linear_layer = 128\n",
    "in_features_decoder_linear_layer = 128\n",
    "max_seq_len = enc_seq_len\n",
    "batch_first = True\n",
    "epochs = 5\n",
    "batch_size=64\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "                input_size=1,\n",
    "                dec_seq_len=enc_seq_len,\n",
    "                batch_first=batch_first\n",
    "                )\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Load the training data\n",
    "training_dataloader = DataLoader(training_data, batch_size= batch_size)\n",
    "# Load the testing data\n",
    "testing_dataloader = DataLoader(testing_data, batch_size= batch_size)\n",
    "\n",
    "# training prediction\n",
    "training_truple = ()\n",
    "# testing prediction\n",
    "testing_truple = ()\n",
    "\n",
    "# Iterate over all epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Iterate over all (x,y) pairs in training dataloader\n",
    "    for i, (src, trg, trg_y) in enumerate(training_dataloader):\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Permute from shape [batch size, seq len, num features] to [seq len, batch size, num features]\n",
    "        if batch_first == False:\n",
    "\n",
    "            shape_before = src.shape\n",
    "            src = src.permute(1, 0, 2)\n",
    "            print(\"src shape changed from {} to {}\".format(shape_before, src.shape))\n",
    "\n",
    "            shape_before = trg.shape\n",
    "            trg = trg.permute(1, 0, 2)\n",
    "            print(\"src shape changed from {} to {}\".format(shape_before, src.shape))\n",
    "\n",
    "\n",
    "        # Make src mask for decoder with size:\n",
    "        # [batch_size*n_heads, output_sequence_length, enc_seq_len]\n",
    "        src_mask = generate_square_subsequent_mask(\n",
    "            dim1=output_sequence_length,\n",
    "            dim2=enc_seq_len,\n",
    "            device = device \n",
    "            \n",
    "            )\n",
    "\n",
    "        # Make tgt mask for decoder with size:\n",
    "        # [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "        tgt_mask = generate_square_subsequent_mask( \n",
    "            dim1=output_sequence_length,\n",
    "            dim2=output_sequence_length,\n",
    "            device = device\n",
    "            )\n",
    "        # make prediction\n",
    "        prediction = model(\n",
    "            src=src,\n",
    "            tgt=trg,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "            )\n",
    "        # Compute and backprop loss\n",
    "        loss = criterion(trg_y, prediction)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Take optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i%100 == 0):\n",
    "            print(f'training_loss: {loss}')\n",
    "            \n",
    "        if (epoch == epochs-1):\n",
    "            training_truple += (prediction, )\n",
    "       \n",
    "    print(f' epoch:{epoch}    training_loss: {loss}')\n",
    "        \n",
    "    # Iterate over all (x,y) pairs in validation dataloader\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for j, (src, _, tgt_y) in enumerate(testing_dataloader):\n",
    "\n",
    "            prediction = run_encoder_decoder_inference(\n",
    "                model=model, \n",
    "                src=src, \n",
    "                batch_size=batch_size,\n",
    "                device = device,\n",
    "                output_sequence_length = output_sequence_length\n",
    "                )\n",
    "            loss = criterion(tgt_y, prediction)\n",
    "            \n",
    "            if (j%100 == 0):\n",
    "                print(f'testing_loss: {loss}')\n",
    "                \n",
    "            if (epoch == epochs-1):\n",
    "                testing_truple += (prediction, )\n",
    "                \n",
    "    print(f' epoch:{epoch}    testing_loss: {loss}')\n",
    "    print(tgt_y[0],\"::::\", prediction[3])\n",
    "    print(tgt_y[3],\"::::\", prediction[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0446ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_Sigdata = torch.cat(testing_truple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "038c76ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200000, 10, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_Sigdata.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cab4c0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(testing_Sigdata.size()[0]*0.6)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63fe71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_Sigdata = testing_Sigdata.detach()\n",
    "#training_Sigdata = training_Sigdata.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bda4ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_Sigdata1 = testing_Sigdata.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7592d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_Sigdata = testing_Sigdata1[size:]\n",
    "training_Sigdata = testing_Sigdata1[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb514e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80000, 10, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_Sigdata.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b9e1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for classifier models.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        data: torch.tensor,\n",
    "        label: torch.tensor\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "        print(\"data size = {}\".format(data.size()))\n",
    "        print(\"target size = {}\".format(label.size()))\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data.size()[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple with 3 elements:\n",
    "        1) src (the encoder input)\n",
    "        2) trg (the decoder input)\n",
    "        3) trg_y (the target)\n",
    "        \"\"\"\n",
    "        #print(self.data.size())\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aefe13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size = torch.Size([120000, 10, 1])\n",
      "target size = torch.Size([120000, 1])\n"
     ]
    }
   ],
   "source": [
    "training_Sigclass = ClassifierDataset(\n",
    "    data = training_Sigdata ,\n",
    "    label = test_labels[:size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f913c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size = torch.Size([80000, 10, 1])\n",
      "target size = torch.Size([80000, 1])\n"
     ]
    }
   ],
   "source": [
    "testing_Sigclass = ClassifierDataset(\n",
    "    data = testing_Sigdata,\n",
    "    label = test_labels[size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e5d94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        input_size: int,\n",
    "        dec_seq_len: int,\n",
    "        batch_first: bool,\n",
    "        dim_val: int=256,  \n",
    "        n_encoder_layers: int=4,\n",
    "        n_heads: int=16,\n",
    "        dropout_encoder: float=0.2, \n",
    "        dropout_pos_enc: float=0.1,\n",
    "        dim_feedforward_encoder: int= 1024\n",
    "        ): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce \n",
    "                     outputs of dimension dim_val\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "            \n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "            \n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer \n",
    "                                     of the encoder\n",
    "            \n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    Most of the time, this will be 1 .\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=dim_val \n",
    "            )\n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=dim_val, \n",
    "            out_features=input_size\n",
    "            )\n",
    "        \n",
    "        self.final_linear_mapping = nn.Linear(\n",
    "            in_features=dec_seq_len, \n",
    "            out_features=input_size\n",
    "            )\n",
    "        \n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoder(\n",
    "            d_model=dim_val,\n",
    "            dropout=dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        # Creating the encoder layer \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerEncoder\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \n",
    "        Args:\n",
    "            src: the encoder's output sequence. Shape: \n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if \n",
    "                 batch_first=True, where S is the source sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    " \n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
    "        #print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "\n",
    "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "            )\n",
    "        #print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "        \n",
    "        # Pass through linear mapping\n",
    "        linear_output = self.linear_mapping(src) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): output size after linear_mapping = {}\".format(linear_output.size()))\n",
    "        \n",
    "        linear_output = linear_output.squeeze(-1)\n",
    "        #print(\"From model.forward(): output size after unsqeeze = {}\".format(linear_output.size()))\n",
    "        \n",
    "        # Pass through linear mapping\n",
    "        final_linear_output = self.final_linear_mapping(linear_output) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): output size after final linear_mapping = {}\".format(final_linear_output.size()))\n",
    "        \n",
    "        # Final output\n",
    "        final_output = self.output(final_linear_output)\n",
    "        #print(\"From model.forward(): final output size = {}\".format(final_output.size()))\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ab501e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.6744452714920044\n",
      "training_loss: 0.6740921139717102\n",
      "training_loss: 0.7018966674804688\n",
      "training_loss: 0.5931987762451172\n",
      "training_loss: 0.5693020224571228\n",
      "training_loss: 0.49096691608428955\n",
      "training_loss: 0.6100394129753113\n",
      "training_loss: 0.5181993842124939\n",
      "training_loss: 0.5611191987991333\n",
      "training_loss: 0.5110502243041992\n",
      "training_loss: 0.6716484427452087\n",
      "training_loss: 0.48498931527137756\n",
      "training_loss: 0.5288876295089722\n",
      "training_loss: 0.5086191892623901\n",
      "training_loss: 0.5472332835197449\n",
      "training_loss: 0.6128122806549072\n",
      "training_loss: 0.5902581810951233\n",
      "training_loss: 0.44617122411727905\n",
      "training_loss: 0.4258868396282196\n",
      " epoch:0    training_loss: 0.5401057004928589\n",
      "testing_loss: 0.6146404147148132\n",
      "testing_loss: 0.5418919324874878\n",
      "testing_loss: 0.4416505694389343\n",
      "testing_loss: 0.5880805253982544\n",
      "testing_loss: 0.5195253491401672\n",
      "testing_loss: 0.5172178149223328\n",
      "testing_loss: 0.6430425643920898\n",
      "testing_loss: 0.5694857239723206\n",
      "testing_loss: 0.5679985284805298\n",
      "testing_loss: 0.5247341394424438\n",
      "testing_loss: 0.5472404956817627\n",
      "testing_loss: 0.4906688928604126\n",
      "testing_loss: 0.6414299607276917\n",
      " epoch:0    testing_loss: 0.6176270842552185\n",
      "training_loss: 0.6104919910430908\n",
      "training_loss: 0.6651424765586853\n",
      "training_loss: 0.7342450618743896\n",
      "training_loss: 0.5695726871490479\n",
      "training_loss: 0.5504580736160278\n",
      "training_loss: 0.46702390909194946\n",
      "training_loss: 0.5810959935188293\n",
      "training_loss: 0.5007027387619019\n",
      "training_loss: 0.5349687337875366\n",
      "training_loss: 0.5026655197143555\n",
      "training_loss: 0.6518520712852478\n",
      "training_loss: 0.5066192150115967\n",
      "training_loss: 0.5401265621185303\n",
      "training_loss: 0.49791690707206726\n",
      "training_loss: 0.5397900938987732\n",
      "training_loss: 0.6160815954208374\n",
      "training_loss: 0.5966644287109375\n",
      "training_loss: 0.4081718623638153\n",
      "training_loss: 0.4326188862323761\n",
      " epoch:1    training_loss: 0.5121791362762451\n",
      "testing_loss: 0.5993251800537109\n",
      "testing_loss: 0.5498998165130615\n",
      "testing_loss: 0.4491581916809082\n",
      "testing_loss: 0.5776435136795044\n",
      "testing_loss: 0.5065147280693054\n",
      "testing_loss: 0.5181595087051392\n",
      "testing_loss: 0.6312265992164612\n",
      "testing_loss: 0.5594984889030457\n",
      "testing_loss: 0.5627685189247131\n",
      "testing_loss: 0.5266117453575134\n",
      "testing_loss: 0.5521097183227539\n",
      "testing_loss: 0.482110857963562\n",
      "testing_loss: 0.6259238719940186\n",
      " epoch:1    testing_loss: 0.6015729904174805\n",
      "training_loss: 0.593694806098938\n",
      "training_loss: 0.6622828841209412\n",
      "training_loss: 0.7206131219863892\n",
      "training_loss: 0.5608989000320435\n",
      "training_loss: 0.5527932047843933\n",
      "training_loss: 0.4704248607158661\n",
      "training_loss: 0.5761187672615051\n",
      "training_loss: 0.4971045255661011\n",
      "training_loss: 0.5382176637649536\n",
      "training_loss: 0.5002626180648804\n",
      "training_loss: 0.6578067541122437\n",
      "training_loss: 0.5089155435562134\n",
      "training_loss: 0.5502822399139404\n",
      "training_loss: 0.49555104970932007\n",
      "training_loss: 0.5270678997039795\n",
      "training_loss: 0.6107046008110046\n",
      "training_loss: 0.5953459739685059\n",
      "training_loss: 0.40615400671958923\n",
      "training_loss: 0.43276622891426086\n",
      " epoch:2    training_loss: 0.4868561625480652\n",
      "testing_loss: 0.6134181022644043\n",
      "testing_loss: 0.5396869778633118\n",
      "testing_loss: 0.44881850481033325\n",
      "testing_loss: 0.5844536423683167\n",
      "testing_loss: 0.4854649007320404\n",
      "testing_loss: 0.5044540762901306\n",
      "testing_loss: 0.631321132183075\n",
      "testing_loss: 0.5681427717208862\n",
      "testing_loss: 0.5567355155944824\n",
      "testing_loss: 0.5212722420692444\n",
      "testing_loss: 0.5607123374938965\n",
      "testing_loss: 0.4902923107147217\n",
      "testing_loss: 0.6183223128318787\n",
      " epoch:2    testing_loss: 0.6003485918045044\n",
      "training_loss: 0.5978074669837952\n",
      "training_loss: 0.6594066619873047\n",
      "training_loss: 0.7150224447250366\n",
      "training_loss: 0.5566056966781616\n",
      "training_loss: 0.5609252452850342\n",
      "training_loss: 0.47104018926620483\n",
      "training_loss: 0.5754339694976807\n",
      "training_loss: 0.4965559244155884\n",
      "training_loss: 0.5401235818862915\n",
      "training_loss: 0.49974143505096436\n",
      "training_loss: 0.6541653871536255\n",
      "training_loss: 0.5119853019714355\n",
      "training_loss: 0.5545362234115601\n",
      "training_loss: 0.4876161217689514\n",
      "training_loss: 0.5183504819869995\n",
      "training_loss: 0.6035020351409912\n",
      "training_loss: 0.5921264290809631\n",
      "training_loss: 0.4049152433872223\n",
      "training_loss: 0.4336867928504944\n",
      " epoch:3    training_loss: 0.4919905662536621\n",
      "testing_loss: 0.6142507791519165\n",
      "testing_loss: 0.5368786454200745\n",
      "testing_loss: 0.4485083818435669\n",
      "testing_loss: 0.5935177206993103\n",
      "testing_loss: 0.48917660117149353\n",
      "testing_loss: 0.4984952211380005\n",
      "testing_loss: 0.6311415433883667\n",
      "testing_loss: 0.570361852645874\n",
      "testing_loss: 0.5521888732910156\n",
      "testing_loss: 0.5205107927322388\n",
      "testing_loss: 0.5621203184127808\n",
      "testing_loss: 0.4867853820323944\n",
      "testing_loss: 0.6206631660461426\n",
      " epoch:3    testing_loss: 0.5989674925804138\n",
      "training_loss: 0.6019688248634338\n",
      "training_loss: 0.652788519859314\n",
      "training_loss: 0.711242139339447\n",
      "training_loss: 0.5628167390823364\n",
      "training_loss: 0.5684577226638794\n",
      "training_loss: 0.4682512581348419\n",
      "training_loss: 0.5730897188186646\n",
      "training_loss: 0.49867069721221924\n",
      "training_loss: 0.5376393795013428\n",
      "training_loss: 0.4993012547492981\n",
      "training_loss: 0.6479823589324951\n",
      "training_loss: 0.5161293745040894\n",
      "training_loss: 0.5533639192581177\n",
      "training_loss: 0.48457014560699463\n",
      "training_loss: 0.512165904045105\n",
      "training_loss: 0.5949877500534058\n",
      "training_loss: 0.594186007976532\n",
      "training_loss: 0.4013994336128235\n",
      "training_loss: 0.4331599473953247\n",
      " epoch:4    training_loss: 0.4901598393917084\n",
      "testing_loss: 0.6125410795211792\n",
      "testing_loss: 0.5334435105323792\n",
      "testing_loss: 0.447429358959198\n",
      "testing_loss: 0.5990033149719238\n",
      "testing_loss: 0.49218615889549255\n",
      "testing_loss: 0.4936157464981079\n",
      "testing_loss: 0.6281452178955078\n",
      "testing_loss: 0.5716426968574524\n",
      "testing_loss: 0.5471891760826111\n",
      "testing_loss: 0.5199304819107056\n",
      "testing_loss: 0.5627714395523071\n",
      "testing_loss: 0.48563045263290405\n",
      "testing_loss: 0.6213078498840332\n",
      " epoch:4    testing_loss: 0.5962868928909302\n"
     ]
    }
   ],
   "source": [
    "## Params\n",
    "dim_val = 256\n",
    "n_heads = 16\n",
    "n_encoder_layers = 4\n",
    "dec_seq_len = 10 # length of input given to decoder\n",
    "enc_seq_len = 10 # length of input given to encoder\n",
    "in_features_encoder_linear_layer = 256\n",
    "max_seq_len = enc_seq_len\n",
    "batch_first = True\n",
    "epochs = 5\n",
    "batch_size=64\n",
    "\n",
    "classifier_model = TimeSeriesClassifier(\n",
    "                input_size=1,\n",
    "                dec_seq_len=enc_seq_len,\n",
    "                batch_first=batch_first\n",
    "                )\n",
    "\n",
    "classifier_model = classifier_model.to(device)\n",
    "\n",
    "optimizer1 = torch.optim.Adam(classifier_model.parameters(), lr=0.0001)\n",
    "criterion1 = torch.nn.BCELoss()\n",
    "\n",
    "# Load the training data\n",
    "training_Sigdataloader = DataLoader(training_Sigclass, batch_size= batch_size)\n",
    "# Load the testing data\n",
    "testing_Sigdataloader = DataLoader(testing_Sigclass, batch_size= batch_size)\n",
    "\n",
    "# training prediction\n",
    "training_truple = ()\n",
    "training_truple_labels = ()\n",
    "# testing prediction\n",
    "testing_truple = ()\n",
    "testing_truple_labels = ()\n",
    "\n",
    "\n",
    "# Iterate over all epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Iterate over all (x,y) pairs in training dataloader\n",
    "    for i, (data, labels) in enumerate(training_Sigdataloader):\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer1.zero_grad()\n",
    "\n",
    "        # Permute from shape [batch size, seq len, num features] to [seq len, batch size, num features]\n",
    "        if batch_first == False:\n",
    "\n",
    "            shape_before = data.shape\n",
    "            data = data.permute(1, 0, 2)\n",
    "            print(\"src shape changed from {} to {}\".format(shape_before, data.shape))\n",
    "\n",
    "            shape_before = labels.shape\n",
    "            labels = labels.permute(1, 0)\n",
    "            print(\"labels shape changed from {} to {}\".format(shape_before, labels.shape))\n",
    "\n",
    "\n",
    "        # make prediction\n",
    "        prediction = classifier_model(\n",
    "            src=data\n",
    "            )\n",
    "        # Compute and backprop loss\n",
    "        loss = criterion1(prediction, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Take optimizer step\n",
    "        optimizer1.step()\n",
    "        \n",
    "        if (i%100 == 0):\n",
    "            print(f'training_loss: {loss}')\n",
    "        \n",
    "        if (epoch == epochs-1):\n",
    "                training_truple += (prediction, )\n",
    "                training_truple_labels +=(labels, )\n",
    "        \n",
    "    print(f' epoch:{epoch}    training_loss: {loss}')\n",
    "        \n",
    "    # Iterate over all (x,y) pairs in validation dataloader\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for j, (data_, labels_) in enumerate(testing_Sigdataloader):\n",
    "\n",
    "            prediction_ = classifier_model(\n",
    "                src=data_\n",
    "            )\n",
    "            loss_ = criterion1(prediction_, labels_)\n",
    "            \n",
    "            if (j%100 == 0):\n",
    "                print(f'testing_loss: {loss_}')\n",
    "            \n",
    "            if (epoch == epochs-1):\n",
    "                testing_truple += (prediction_, )\n",
    "                testing_truple_labels +=(labels_, )\n",
    "                \n",
    "    print(f' epoch:{epoch}    testing_loss: {loss_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03be0379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80000, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_hat = torch.cat(testing_truple)\n",
    "testing_hat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e9719ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80000, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_org = torch.cat(testing_truple_labels)\n",
    "testing_org.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51ba434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = testing_org.data.cpu().numpy()\n",
    "predictions = testing_hat.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc075716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5353a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test labels are: [1. 1. 0. ... 0. 1. 0.]\n",
      "[[0.1585871 ]\n",
      " [0.43080148]\n",
      " [0.05791693]\n",
      " ...\n",
      " [0.1258061 ]\n",
      " [0.4523542 ]\n",
      " [0.10923808]]\n",
      "[[0.76220936]\n",
      " [0.70070106]\n",
      " [0.16777162]\n",
      " ...\n",
      " [0.7820135 ]\n",
      " [0.78641355]\n",
      " [0.6761254 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'test labels are: {y_test[:,0]}')\n",
    "\n",
    "# Create separate arrays for background and signal predictions\n",
    "bkg_preds = predictions[y_test[:,0] == 0]\n",
    "sig_preds = predictions[y_test[:,0] == 1]\n",
    "\n",
    "print(bkg_preds)\n",
    "print(sig_preds)\n",
    "# Plot the predicted probabilities\n",
    "plt.hist(bkg_preds, bins=50, label='Background', alpha=0.5)\n",
    "plt.hist(sig_preds, bins=50, label='Signal', alpha=0.5, color='green')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.legend(loc='upper center')\n",
    "#plt.ylim(top=100)\n",
    "#plt.show()\n",
    "plt.savefig(\"bkgVsSig_v2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a86646d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.xlim([0.9, 1])\n",
    "#plt.ylim([0.999, 1.005])\n",
    "#plt.show()\n",
    "plt.savefig(\"ROC_v2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e852f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### precision recall curve\n",
    "# Calculate precision-recall curve and average precision\n",
    "precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "average_precision = average_precision_score(y_test, predictions)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_prediction = predictions > 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(binary_prediction == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601ec6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
